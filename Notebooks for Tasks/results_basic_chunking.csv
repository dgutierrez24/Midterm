question,contexts,answer,ground_truth,faithfulness,answer_relevancy,context_recall,context_precision,answer_correctness
How do healthcare navigators assist individuals in finding and obtaining healthcare coverage?,"[""HUMAN ALTERNATIVES, \nCONSIDERATION, AND \nFALLBACK \nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\nReal-life examples of how these principles can become reality, through laws, policies, and practical \ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \nHealthcare “navigators” help people find their way through online signup forms to choose \nand obtain healthcare. A Navigator is “an individual or organization that's trained and able to help \nconsumers, small businesses, and their employees as they look for health coverage options through the \nMarketplace (a government web site), including completing eligibility and enrollment forms.”106 For""
 'Communities Through the Federal Government:\nhttps://www.whitehouse.gov/briefing-room/presidential-actions/2021/01/20/executive-order\xad\nadvancing-racial-equity-and-support-for-underserved-communities-through-the-federal-government/\n106. HealthCare.gov. Navigator - HealthCare.gov Glossary. Accessed May 2, 2022.\nhttps://www.healthcare.gov/glossary/navigator/\n72'
 'ENDNOTES\n107. Centers for Medicare & Medicaid Services. Biden-Harris Administration Quadruples the Number\nof Health Care Navigators Ahead of HealthCare.gov Open Enrollment Period. Aug. 27, 2021.\nhttps://www.cms.gov/newsroom/press-releases/biden-harris-administration-quadruples-number\xad\nhealth-care-navigators-ahead-healthcaregov-open\n108. See, e.g., McKinsey & Company. The State of Customer Care in 2022. July 8, 2022. https://\nwww.mckinsey.com/business-functions/operations/our-insights/the-state-of-customer-care-in-2022;\nSara Angeles. Customer Service Solutions for Small Businesses. Business News Daily.\nJun. 29, 2022. https://www.businessnewsdaily.com/7575-customer-service-solutions.html'
 'the 2022 plan year, the Biden-Harris Administration increased funding so that grantee organizations could \n“train and certify more than 1,500 Navigators to help uninsured consumers find affordable and comprehensive \nhealth coverage.”107\nThe customer service industry has successfully integrated automated services such as \nchat-bots and AI-driven call response systems with escalation to a human support \nteam.108 Many businesses now use partially automated customer service platforms that help answer customer \nquestions and compile common problems for human agents to review. These integrated human-AI \nsystems allow companies to provide faster customer care while maintaining human agents to answer']","Healthcare navigators assist individuals by helping them navigate online signup forms to choose and obtain healthcare coverage. They are trained individuals or organizations that guide consumers, small businesses, and their employees in exploring health coverage options through the Marketplace, which is a government website. This includes assisting with completing eligibility and enrollment forms.","Healthcare navigators assist individuals in finding and obtaining healthcare coverage by helping them navigate online signup forms, choose appropriate health coverage options through the Marketplace, and complete eligibility and enrollment forms. In the 2022 plan year, the Biden-Harris Administration increased funding to train and certify more than 1,500 Navigators to assist uninsured consumers in finding affordable and comprehensive health coverage.",1.0,0.9433452626159475,1.0,0.7499999999625,0.8172115551477046
How are AI system performance or assurance criteria measured and demonstrated for conditions similar to deployment settings?,"['MEASURE 2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for \nconditions similar to deployment setting(s). Measures are documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.3-001 Consider baseline model performance on suites of benchmarks when selecting a \nmodel for ﬁne tuning or enhancement with retrieval-augmented generation. \nInformation Security; \nConfabulation \nMS-2.3-002 Evaluate claims of model capabilities using empirically validated methods. \nConfabulation; Information \nSecurity \nMS-2.3-003 Share results of pre-deployment testing with relevant GAI Actors, such as those \nwith system release approval authority. \nHuman-AI Conﬁguration'
 '31 \nMS-2.3-004 \nUtilize a purpose-built testing environment such as NIST Dioptra to empirically \nevaluate GAI trustworthy characteristics. \nCBRN Information or Capabilities; \nData Privacy; Confabulation; \nInformation Integrity; Information \nSecurity; Dangerous, Violent, or \nHateful Content; Harmful Bias and \nHomogenization \nAI Actor Tasks: AI Deployment, TEVV \n \nMEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the \nconditions under which the technology was developed are documented. \nAction ID \nSuggested Action \nRisks \nMS-2.5-001 Avoid extrapolating GAI system performance or capabilities from narrow, non-\nsystematic, and anecdotal assessments.'
 'Assess the general awareness among end users and impacted communities \nabout the availability of these feedback channels. \nHuman-AI Conﬁguration; \nInformation Integrity; Harmful Bias \nand Homogenization \nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, End-Users, Operation and Monitoring, TEVV \n \nMEASURE 4.2: Measurement results regarding AI system trustworthiness in deployment context(s) and across the AI lifecycle are \ninformed by input from domain experts and relevant AI Actors to validate whether the system is performing consistently as \nintended. Results are documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-4.2-001 \nConduct adversarial testing at a regular cadence to map and measure GAI risks,'
 'service providers, including acquisition and procurement due diligence, requests for software bills of \nmaterials (SBOMs), application of service level agreements (SLAs), and statement on standards for \nattestation engagement (SSAE) reports to help with third-party transparency and risk management for \nGAI systems. \nA.1.4. Pre-Deployment Testing \nOverview \nThe diverse ways and contexts in which GAI systems may be developed, used, and repurposed \ncomplicates risk mapping and pre-deployment measurement eﬀorts. Robust test, evaluation, validation, \nand veriﬁcation (TEVV) processes can be iteratively applied – and documented – in early stages of the AI']","AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment settings. The measures are documented, and specific actions are suggested to ensure effective evaluation. For instance:

1. **Baseline Model Performance**: Consider baseline model performance on suites of benchmarks when selecting a model for fine-tuning or enhancement with retrieval-augmented generation (Action ID: MS-2.3-001).

2. **Empirical Validation**: Evaluate claims of model capabilities using empirically validated methods (Action ID: MS-2.3-002).

3. **Pre-Deployment Testing**: Share results of pre-deployment testing with relevant GAI actors, such as those with system release approval authority (Action ID: MS-2.3-003).

4. **Purpose-Built Testing Environment**: Utilize a purpose-built testing environment, such as NIST Dioptra, to empirically evaluate GAI trustworthy characteristics (Action ID: MS-2.3-004).

These actions help ensure that the AI system is valid and reliable, with limitations of generalizability beyond the conditions under which the technology was developed documented (Measure 2.5).","AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment settings by considering baseline model performance on suites of benchmarks when selecting a model for fine tuning or enhancement with retrieval-augmented generation. Claims of model capabilities are evaluated using empirically validated methods, and results of pre-deployment testing are shared with relevant GAI Actors, such as those with system release approval authority.",1.0,0.9249659310159393,1.0,0.999999999975,0.7697495332179093
How can organizations enhance content provenance through structured public feedback in monitoring system capabilities?,"['origins and modiﬁcations. Further narrowing of GAI task deﬁnitions to include provenance data can \nenable organizations to maximize the utility of provenance data and risk management eﬀorts. \nA.1.7. Enhancing Content Provenance through Structured Public Feedback \nWhile indirect feedback methods such as automated error collection systems are useful, they often lack \nthe context and depth that direct input from end users can provide. Organizations can leverage feedback \napproaches described in the Pre-Deployment Testing section to capture input from external sources such \nas through AI red-teaming.  \nIntegrating pre- and post-deployment external feedback into the monitoring process for GAI models and'
 'corresponding applications can help enhance awareness of performance changes and mitigate potential \nrisks and harms from outputs. There are many ways to capture and make use of user feedback – before \nand after GAI systems and digital content transparency approaches are deployed – to gain insights about \nauthentication eﬃcacy and vulnerabilities, impacts of adversarial threats on techniques, and unintended \nconsequences resulting from the utilization of content provenance approaches on users and \ncommunities. Furthermore, organizations can track and document the provenance of datasets to identify \ninstances in which AI-generated data is a potential root cause of performance issues with the GAI \nsystem. \nA.1.8. Incident Disclosure'
 'can serve multiple purposes, including improving data quality and preprocessing, bolstering governance \ndecision making, and enhancing system documentation and debugging practices. When implementing \nfeedback activities, organizations should follow human subjects research requirements and best \npractices such as informed consent and subject compensation.'
 '52 \n• \nMonitoring system capabilities and limitations in deployment through rigorous TEVV processes; \n• \nEvaluating how humans engage, interact with, or adapt to GAI content (especially in decision \nmaking tasks informed by GAI content), and how they react to applied provenance techniques \nsuch as overt disclosures. \nOrganizations can document and delineate GAI system objectives and limitations to identify gaps where \nprovenance data may be most useful. For instance, GAI systems used for content creation may require \nrobust watermarking techniques and corresponding detectors to identify the source of content or \nmetadata recording techniques and metadata management tools and repositories to trace content']","Organizations can enhance content provenance through structured public feedback by integrating pre- and post-deployment external feedback into the monitoring process for Generative AI (GAI) models and their corresponding applications. This approach helps to increase awareness of performance changes and mitigate potential risks and harms from outputs. 

By leveraging feedback methods described in the Pre-Deployment Testing section, organizations can capture direct input from end users, which provides valuable context and depth that automated error collection systems may lack. Additionally, organizations can track and document the provenance of datasets to identify instances where AI-generated data may be a root cause of performance issues. 

Overall, structured public feedback can provide insights into authentication efficacy, vulnerabilities, impacts of adversarial threats, and unintended consequences of content provenance approaches on users and communities, thereby enhancing the overall monitoring of system capabilities.","Organizations can enhance content provenance through structured public feedback in monitoring system capabilities by integrating pre- and post-deployment external feedback into the monitoring process for GAI models and corresponding applications. This can help enhance awareness of performance changes and mitigate potential risks and harms from outputs. By capturing input from external sources, such as through AI red-teaming, organizations can gain valuable insights about authentication efficacy, vulnerabilities, impacts of adversarial threats, and unintended consequences resulting from the utilization of content provenance approaches on users and communities.",0.9375,0.9724288609126721,1.0,0.9166666666361111,0.8304846123276002
How can concerns of model collapse be mitigated by assessing the proportion of synthetic to non-synthetic training data?,"['9 \nand reduced content diversity). Overly homogenized outputs can themselves be incorrect, or they may \nlead to unreliable decision-making or amplify harmful biases. These phenomena can ﬂow from \nfoundation models to downstream models and systems, with the foundation models acting as \n“bottlenecks,” or single points of failure.  \nOverly homogenized content can contribute to “model collapse.” Model collapse can occur when model \ntraining over-relies on synthetic data, resulting in data points disappearing from the distribution of the \nnew model’s outputs. In addition to threatening the robustness of the model overall, model collapse \ncould lead to homogenized outputs, including by amplifying any homogenization from the model used to'
 '37 \nMS-2.11-005 \nAssess the proportion of synthetic to non-synthetic training data and verify \ntraining data is not overly homogenous or GAI-produced to mitigate concerns of \nmodel collapse. \nHarmful Bias and Homogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, End-Users, \nOperation and Monitoring, TEVV \n \nMEASURE 2.12: Environmental impact and sustainability of AI model training and management activities – as identiﬁed in the MAP \nfunction – are assessed and documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.12-001 Assess safety to physical environments when deploying GAI systems. \nDangerous, Violent, or Hateful \nContent'
 'Shevlane, T. et al. (2023) Model evaluation for extreme risks. arXiv. https://arxiv.org/pdf/2305.15324 \nShumailov, I. et al. (2023) The curse of recursion: training on generated data makes models forget. arXiv. \nhttps://arxiv.org/pdf/2305.17493v2 \nSmith, A. et al. (2023) Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language \nModels. PLOS Digital Health. \nhttps://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000388 \nSoice, E. et al. (2023) Can large language models democratize access to dual-use biotechnology? arXiv. \nhttps://arxiv.org/abs/2306.03809 \nSolaiman, I. et al. (2023) The Gradient of Generative AI Release: Methods and Considerations. arXiv. \nhttps://arxiv.org/abs/2302.04844'
 'distinguish human-generated content from AI-generated synthetic content. To help manage and mitigate \nthese risks, digital transparency mechanisms like provenance data tracking can trace the origin and \nhistory of content. Provenance data tracking and synthetic content detection can help facilitate greater \ninformation access about both authentic and synthetic content to users, enabling better knowledge of \ntrustworthiness in AI systems. When combined with other organizational accountability mechanisms, \ndigital content transparency approaches can enable processes to trace negative outcomes back to their \nsource, improve information integrity, and uphold public trust. Provenance data tracking and synthetic']","Concerns of model collapse can be mitigated by assessing the proportion of synthetic to non-synthetic training data because an overly homogenized dataset, particularly one that relies heavily on synthetic data, can lead to a reduction in the diversity of outputs generated by the model. By evaluating this proportion, it is possible to ensure that the training data includes a sufficient amount of non-synthetic, diverse data points. This helps maintain a robust distribution of outputs and prevents the model from becoming overly reliant on synthetic data, which can lead to the disappearance of important data points and ultimately result in model collapse. Therefore, a balanced mix of synthetic and non-synthetic data is crucial for maintaining the integrity and reliability of the model's performance.","Assessing the proportion of synthetic to non-synthetic training data and ensuring that training data is not overly homogenous or GAI-produced can help mitigate concerns of model collapse. This approach helps in preventing the model from becoming too biased or overfitting to the training data, thus improving its generalization and performance.",0.4444444444444444,0.9169802023863586,0.5,0.999999999975,0.6655118504623603
What is the role of the NIST Generative AI Public Working Group in advancing reliable and safe artificial intelligence at NIST?,"['trustworthy development and use of AI. \nAcknowledgments: This report was accomplished with the many helpful comments and contributions \nfrom the community, including the NIST Generative AI Public Working Group, and NIST staﬀ and guest \nresearchers: Chloe Autio, Jesse Dunietz, Patrick Hall, Shomik Jain, Kamie Roberts, Reva Schwartz, Martin \nStanley, and Elham Tabassi. \nNIST Technical Series Policies \nCopyright, Use, and Licensing Statements \nNIST Technical Series Publication Identifier Syntax \nPublication History \nApproved by the NIST Editorial Review Board on 07-25-2024 \nContact Information \nai-inquiries@nist.gov \nNational Institute of Standards and Technology \nAttn: NIST AI Innovation Lab, Information Technology Laboratory'
 '2 \nThis work was informed by public feedback and consultations with diverse stakeholder groups as part of NIST’s \nGenerative AI Public Working Group (GAI PWG). The GAI PWG was an open, transparent, and collaborative \nprocess, facilitated via a virtual workspace, to obtain multistakeholder input on GAI risk management and to \ninform NIST’s approach. \nThe focus of the GAI PWG was limited to four primary considerations relevant to GAI: Governance, Content \nProvenance, Pre-deployment Testing, and Incident Disclosure (further described in Appendix A). As such, the \nsuggested actions in this document primarily address these considerations.'
 'NIST Trustworthy and Responsible AI  \nNIST AI 600-1 \nArtificial Intelligence Risk Management \nFramework: Generative Artificial \nIntelligence Profile \n \n \n \nThis publication is available free of charge from: \nhttps://doi.org/10.6028/NIST.AI.600-1'
 'NIST Trustworthy and Responsible AI  \nNIST AI 600-1 \nArtificial Intelligence Risk Management \nFramework: Generative Artificial \nIntelligence Profile \n \n \n \nThis publication is available free of charge from: \nhttps://doi.org/10.6028/NIST.AI.600-1 \n \nJuly 2024 \n \n \n \n \nU.S. Department of Commerce  \nGina M. Raimondo, Secretary \nNational Institute of Standards and Technology  \nLaurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology']","The NIST Generative AI Public Working Group (GAI PWG) plays a crucial role in advancing reliable and safe artificial intelligence at NIST by facilitating an open, transparent, and collaborative process to gather multistakeholder input on generative AI risk management. This group was informed by public feedback and consultations with diverse stakeholder groups, focusing on four primary considerations relevant to generative AI: Governance, Content Provenance, Pre-deployment Testing, and Incident Disclosure. The suggested actions in the NIST document primarily address these considerations, thereby contributing to the trustworthy development and use of AI.",The answer to given question is not present in context,0.6666666666666666,0.9707605091458605,1.0,0.0,0.9260681569369832
What processes should be established to verify the AI Actors conducting GAI incident response tasks demonstrate and maintain the appropriate skills and training?,"['Human-AI Conﬁguration \nAI Actor Tasks: AI Deployment, AI Design, AI Development, Aﬀected Individuals and Communities, End-Users, Operation and \nMonitoring, TEVV \n \nMANAGE 4.3: Incidents and errors are communicated to relevant AI Actors, including aﬀected communities. Processes for tracking, \nresponding to, and recovering from incidents and errors are followed and documented. \nAction ID \nSuggested Action \nGAI Risks \nMG-4.3-001 \nConduct after-action assessments for GAI system incidents to verify incident \nresponse and recovery processes are followed and eﬀective, including to follow \nprocedures for communicating incidents to relevant AI Actors and where \napplicable, relevant legal and regulatory bodies.  \nInformation Security'
 'responsibilities. These guidelines would help AI system operators identify GAI incidents across the AI \nlifecycle and with AI Actors regardless of role. Documentation and review of third-party inputs and \nplugins for GAI systems is especially important for AI Actors in the context of incident disclosure; LLM \ninputs and content delivered through these plugins is often distributed, with inconsistent or insuﬃcient \naccess control. \nDocumentation practices including logging, recording, and analyzing GAI incidents can facilitate \nsmoother sharing of information with relevant AI Actors. Regular information sharing, change \nmanagement records, version history and metadata can also empower AI Actors responding to and \nmanaging AI incidents.'
 '45 \nMG-4.1-007 \nVerify that AI Actors responsible for monitoring reported issues can eﬀectively \nevaluate GAI system performance including the application of content \nprovenance data tracking techniques, and promptly escalate issues for response. \nHuman-AI Conﬁguration; \nInformation Integrity \nAI Actor Tasks: AI Deployment, Aﬀected Individuals and Communities, Domain Experts, End-Users, Human Factors, Operation and \nMonitoring \n \nMANAGE 4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular \nengagement with interested parties, including relevant AI Actors. \nAction ID \nSuggested Action \nGAI Risks \nMG-4.2-001 Conduct regular monitoring of GAI systems and publish reports detailing the'
 'procedures to address the following: Review and maintenance of policies and \nprocedures to account for newly encountered uses; Review and maintenance of \npolicies and procedures for detection of unanticipated uses; Verify response \nand recovery plans account for the GAI system value chain; Verify response and \nrecovery plans are updated for and include necessary details to communicate \nwith downstream GAI system Actors: Points-of-Contact (POC), Contact \ninformation, notiﬁcation format. \nValue Chain and Component \nIntegration \nAI Actor Tasks: AI Deployment, Operation and Monitoring \n \nMANAGE 2.4: Mechanisms are in place and applied, and responsibilities are assigned and understood, to supersede, disengage, or']","The provided context does not specify the exact processes that should be established to verify that AI Actors conducting GAI incident response tasks demonstrate and maintain the appropriate skills and training. It discusses the importance of incident communication, documentation practices, and monitoring of GAI systems, but does not detail specific training or skill verification processes for AI Actors. Therefore, I don't know the answer to your question.",Processes should be established to verify that the AI Actors conducting GAI incident response tasks demonstrate and maintain the appropriate skills and training. This verification is crucial to ensure effective incident response and mitigate risks associated with AI systems.,0.7142857142857143,0.0,1.0,0.999999999975,0.3594015022045737
What is the purpose of conducting adversarial testing in the context of GAI systems?,"['including tests to address attempts to deceive or manipulate the application of \nprovenance techniques or other misuses. Identify vulnerabilities and \nunderstand potential misuse scenarios and unintended outputs. \nInformation Integrity; Information \nSecurity \nMS-4.2-002 \nEvaluate GAI system performance in real-world scenarios to observe its \nbehavior in practical environments and reveal issues that might not surface in \ncontrolled and optimized testing environments. \nHuman-AI Conﬁguration; \nConfabulation; Information \nSecurity \nMS-4.2-003 \nImplement interpretability and explainability methods to evaluate GAI system \ndecisions and verify alignment with intended purpose. \nInformation Integrity; Harmful Bias \nand Homogenization \nMS-4.2-004'
 'under which the GAI system will be deployed. Field style tests can be adapted from a focus on user \npreferences and experiences towards AI risks and impacts – both negative and positive. When carried \nout with large groups of users, these tests can provide estimations of the likelihood of risks and impacts \nin real world interactions. \nOrganizations may also collect feedback on outcomes, harms, and user experience directly from users in \nthe production environment after a model has been released, in accordance with human subject \nstandards such as informed consent and compensation. Organizations should follow applicable human \nsubjects research requirements, and best practices such as informed consent and subject compensation,'
 '31 \nMS-2.3-004 \nUtilize a purpose-built testing environment such as NIST Dioptra to empirically \nevaluate GAI trustworthy characteristics. \nCBRN Information or Capabilities; \nData Privacy; Confabulation; \nInformation Integrity; Information \nSecurity; Dangerous, Violent, or \nHateful Content; Harmful Bias and \nHomogenization \nAI Actor Tasks: AI Deployment, TEVV \n \nMEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability beyond the \nconditions under which the technology was developed are documented. \nAction ID \nSuggested Action \nRisks \nMS-2.5-001 Avoid extrapolating GAI system performance or capabilities from narrow, non-\nsystematic, and anecdotal assessments.'
 'service providers, including acquisition and procurement due diligence, requests for software bills of \nmaterials (SBOMs), application of service level agreements (SLAs), and statement on standards for \nattestation engagement (SSAE) reports to help with third-party transparency and risk management for \nGAI systems. \nA.1.4. Pre-Deployment Testing \nOverview \nThe diverse ways and contexts in which GAI systems may be developed, used, and repurposed \ncomplicates risk mapping and pre-deployment measurement eﬀorts. Robust test, evaluation, validation, \nand veriﬁcation (TEVV) processes can be iteratively applied – and documented – in early stages of the AI']","The context provided does not explicitly mention the purpose of conducting adversarial testing in the context of GAI systems. However, it does discuss the importance of evaluating GAI system performance in real-world scenarios, identifying vulnerabilities, understanding potential misuse scenarios, and ensuring information integrity and security. Adversarial testing could be inferred to be a method to address these concerns by simulating attempts to deceive or manipulate the system, thereby revealing weaknesses and ensuring the system's robustness against misuse. 

If you need a more specific answer regarding adversarial testing, I don't have that information in the provided context.","The purpose of conducting adversarial testing in the context of GAI systems is to map and measure GAI risks, including attempts to deceive or manipulate provenance techniques or other misuses. This testing helps identify vulnerabilities, understand potential misuse scenarios, and reveal unintended outputs.",0.6,0.0,1.0,0.999999999975,0.5850624060753244
"How have advocates, researchers, and government organizations proposed principles for the ethical use of AI and other automated systems?","['lems with legislation, and some courts extending longstanding statutory protections to new and emerging tech\xad\nnologies. There are companies working to incorporate additional protections in their design and use of auto\xad\nmated systems, and researchers developing innovative guardrails. Advocates, researchers, and government \norganizations have proposed principles for the ethical use of AI and other automated systems. These include \nthe Organization for Economic Co-operation and Development’s (OECD’s) 2019 Recommendation on Artificial \nIntelligence, which includes principles for responsible stewardship of trustworthy AI and which the United'
 'SAFE AND EFFECTIVE \nSYSTEMS \nHOW THESE PRINCIPLES CAN MOVE INTO PRACTICE\nReal-life examples of how these principles can become reality, through laws, policies, and practical \ntechnical and sociotechnical approaches to protecting rights, opportunities, and access. \xad\nSome U.S government agencies have developed specific frameworks for ethical use of AI \nsystems. The Department of Energy (DOE) has activated the AI Advancement Council that oversees coordina-\ntion and advises on implementation of the DOE AI Strategy and addresses issues and/or escalations on the \nethical use and development of AI systems.20 The Department of Defense has adopted Artificial Intelligence'
 'context in which automated systems are being utilized. In some circumstances, application of these principles \nin whole or in part may not be appropriate given the intended use of automated systems to achieve government \nagency missions. Future sector-specific guidance will likely be necessary and important for guiding the use of \nautomated systems in certain settings such as AI systems used as part of school building security or automated \nhealth diagnostic systems. \nThe Blueprint for an AI Bill of Rights recognizes that law enforcement activities require a balancing of \nequities, for example, between the protection of sensitive law enforcement information and the principle of'
 ""Ethical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its national \nsecurity and defense activities.21 Similarly, the U.S. Intelligence Community (IC) has developed the Principles \nof Artificial Intelligence Ethics for the Intelligence Community to guide personnel on whether and how to \ndevelop and use AI in furtherance of the IC's mission, as well as an AI Ethics Framework to help implement \nthese principles.22\nThe National Science Foundation (NSF) funds extensive research to help foster the \ndevelopment of automated systems that adhere to and advance their safety, security and \neffectiveness. Multiple NSF programs support research that directly addresses many of these principles:""]","Advocates, researchers, and government organizations have proposed principles for the ethical use of AI and other automated systems through various frameworks and recommendations. For instance, the Organization for Economic Co-operation and Development (OECD) issued a 2019 Recommendation on Artificial Intelligence, which outlines principles for responsible stewardship of trustworthy AI. Additionally, U.S. government agencies, such as the Department of Energy and the Department of Defense, have developed specific frameworks and councils to oversee the ethical use and development of AI systems. The National Science Foundation also funds research aimed at fostering the development of automated systems that adhere to principles of safety, security, and effectiveness. These efforts reflect a collaborative approach to establishing ethical guidelines for AI technologies.","Advocates, researchers, and government organizations have proposed principles for the ethical use of AI and other automated systems. These include the Organization for Economic Co-operation and Development’s (OECD’s) 2019 Recommendation on Artificial Intelligence, which includes principles for responsible stewardship of trustworthy AI and which the United States adopted, and Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government, which sets out principles that govern the federal government’s use of AI.",1.0,0.9565035682277857,0.3333333333333333,0.999999999975,0.8873484819986712
How can a data breach lead to potential identity theft?,"[""DATA PRIVACY \nWHY THIS PRINCIPLE IS IMPORTANT\nThis section provides a brief summary of the problems which the principle seeks to address and protect \nagainst, including illustrative examples. \n•\nAn insurer might collect data from a person's social media presence as part of deciding what life\ninsurance rates they should be offered.64\n•\nA data broker harvested large amounts of personal data and then suffered a breach, exposing hundreds of\nthousands of people to potential identity theft. 65\n•\nA local public housing authority installed a facial recognition system at the entrance to housing complexes to\nassist law enforcement with identifying individuals viewed via camera when police reports are filed, leading""
 'sensitive data about an individual (such as disability-related data, genomic data, biometric data, behavioral data, \ngeolocation data, data related to interaction with the criminal justice system, relationship history and legal status such \nas custody and divorce information, and home, work, or school environmental data); or have the reasonable potential \nto be used in ways that are likely to expose individuals to meaningful harm, such as a loss of privacy or financial harm \ndue to identity theft. Data and metadata generated by or about those who are not yet legal adults is also sensitive, even \nif not related to a sensitive domain. Such data includes, but is not limited to, numerical, text, image, audio, or video'
 'harm, such as a loss of privacy or financial harm due to identity theft. Data and metadata generated by or about \nthose who are not yet legal adults is also sensitive, even if not related to a sensitive domain. Such data includes, \nbut is not limited to, numerical, text, image, audio, or video data. \nSENSITIVE DOMAINS: “Sensitive domains” are those in which activities being conducted can cause material \nharms, including significant adverse effects on human rights such as autonomy and dignity, as well as civil liber\xad\nties and civil rights. Domains that have historically been singled out as deserving of enhanced data protections \nor where such enhanced protections are reasonably expected by the public include, but are not limited to,'
 'consumers’ permission or knowledge.60 Moreover, there is a risk that inaccurate and faulty data can be used to \nmake decisions about their lives, such as whether they will qualify for a loan or get a job. Use of surveillance \ntechnologies has increased in schools and workplaces, and, when coupled with consequential management and \nevaluation decisions, it is leading to mental health harms such as lowered self-confidence, anxiety, depression, and \na reduced ability to use analytical reasoning.61 Documented patterns show that personal data is being aggregated by \ndata brokers to profile communities in harmful ways.62 The impact of all this data harvesting is corrosive,']","A data breach can lead to potential identity theft when personal data is exposed to unauthorized individuals. For example, if a data broker collects and stores large amounts of personal data and then suffers a breach, that exposed data can be accessed by malicious actors. This data may include sensitive information such as social security numbers, financial details, or other identifying information, which can be used to impersonate individuals, access their accounts, or commit fraud. The context mentions that a data broker's breach exposed hundreds of thousands of people to potential identity theft, highlighting the risks associated with inadequate data protection.","A data breach can lead to potential identity theft when a data broker harvests large amounts of personal data and then suffers a breach, exposing hundreds of thousands of people to the risk of identity theft.",0.5,0.9307748238152814,1.0,0.9166666666361111,0.4787312212941231
What is the purpose of the Blueprint for an AI Bill of Rights released by the White House Office of Science and Technology Policy?,"['About this Document \nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People was \npublished by the White House Office of Science and Technology Policy in October 2022. This framework was \nreleased one year after OSTP announced the launch of a process to develop “a bill of rights for an AI-powered \nworld.” Its release follows a year of public engagement to inform this initiative. The framework is available \nonline at: https://www.whitehouse.gov/ostp/ai-bill-of-rights \nAbout the Office of Science and Technology Policy \nThe Office of Science and Technology Policy (OSTP) was established by the National Science and Technology'
 'Legal Disclaimer \nThe Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People is a white paper \npublished by the White House Office of Science and Technology Policy. It is intended to support the \ndevelopment of policies and practices that protect civil rights and promote democratic values in the building, \ndeployment, and governance of automated systems. \nThe Blueprint for an AI Bill of Rights is non-binding and does not constitute U.S. government policy. It \ndoes not supersede, modify, or direct an interpretation of any existing statute, regulation, policy, or \ninternational instrument. It does not constitute binding guidance for the public or Federal agencies and'
 'ABOUT THIS FRAMEWORK\xad\xad\xad\xad\xad\nThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the \ndesign, use, and deployment of automated systems to protect the rights of the American public in the age of \nartificial intel-ligence. Developed through extensive consultation with the American public, these principles are \na blueprint for building and deploying automated systems that are aligned with democratic values and protect \ncivil rights, civil liberties, and privacy. The Blueprint for an AI Bill of Rights includes this Foreword, the five \nprinciples, notes on Applying the The Blueprint for an AI Bill of Rights, and a Technical Companion that gives'
 'basis for so many more rights that we have come to take for granted that are ingrained in the fabric of this \ncountry.”2\nTo advance President Biden’s vision, the White House Office of Science and Technology Policy has identified \nfive principles that should guide the design, use, and deployment of automated systems to protect the American \npublic in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society that \nprotects all people from these threats—and uses technologies in ways that reinforce our highest values. \nResponding to the experiences of the American public, and informed by insights from researchers,']","The purpose of the Blueprint for an AI Bill of Rights, released by the White House Office of Science and Technology Policy, is to support the development of policies and practices that protect civil rights and promote democratic values in the building, deployment, and governance of automated systems. It aims to guide the design, use, and deployment of these systems to protect the rights of the American public in the age of artificial intelligence, ensuring that technologies are aligned with democratic values and safeguard civil rights, civil liberties, and privacy.","The purpose of the Blueprint for an AI Bill of Rights is to support the development of policies and practices that protect civil rights and promote democratic values in the building, deployment, and governance of automated systems.",1.0,0.9624758306492994,1.0,0.999999999975,0.9947894631028755
How to manage risks of GAI with third-party components and human misuse?,"['GAI resources; Apply organizational risk tolerances to ﬁne-tuned third-party \nmodels; Apply organizational risk tolerance to existing third-party models \nadapted to a new domain; Reassess risk measurements after ﬁne-tuning third-\nparty GAI models. \nValue Chain and Component \nIntegration; Intellectual Property \nMG-3.1-002 \nTest GAI system value chain risks (e.g., data poisoning, malware, other software \nand hardware vulnerabilities; labor practices; data privacy and localization \ncompliance; geopolitical alignment). \nData Privacy; Information Security; \nValue Chain and Component \nIntegration; Harmful Bias and \nHomogenization \nMG-3.1-003 \nRe-assess model risks after ﬁne-tuning or retrieval-augmented generation'
 'addressing third-party considerations. \nThird party GAI integrations may give rise to increased intellectual property, data privacy, or information \nsecurity risks, pointing to the need for clear guidelines for transparency and risk management regarding \nthe collection and use of third-party data for model inputs. Organizations may consider varying risk \ncontrols for foundation models, ﬁne-tuned models, and embedded tools, enhanced processes for \ninteracting with external GAI technologies or service providers. Organizations can apply standard or \nexisting risk controls and processes to proprietary or open-source GAI technologies, data, and third-party'
 'GOVERN 6.2: Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be \nhigh-risk. \nAction ID \nSuggested Action \nGAI Risks \nGV-6.2-001 \nDocument GAI risks associated with system value chain to identify over-reliance \non third-party data and to identify fallbacks. \nValue Chain and Component \nIntegration \nGV-6.2-002 \nDocument incidents involving third-party GAI data and systems, including open-\ndata and open-source software. \nIntellectual Property; Value Chain \nand Component Integration'
 'Information Security \n \nAI Actor Tasks: AI Deployment, Governance and Oversight, Operation and Monitoring \n \nMANAGE 3.1: AI risks and beneﬁts from third-party resources are regularly monitored, and risk controls are applied and \ndocumented. \nAction ID \nSuggested Action \nGAI Risks \nMG-3.1-001 \nApply organizational risk tolerances and controls (e.g., acquisition and \nprocurement processes; assessing personnel credentials and qualiﬁcations, \nperforming background checks; ﬁltering GAI input and outputs, grounding, ﬁne \ntuning, retrieval-augmented generation) to third-party GAI resources: Apply \norganizational risk tolerance to the utilization of third-party datasets and other']","To manage the risks of Generative AI (GAI) with third-party components and human misuse, organizations can implement the following strategies based on the provided context:

1. **Apply Organizational Risk Tolerances**: Establish and apply organizational risk tolerances to third-party GAI resources. This includes assessing the risks associated with the acquisition and use of third-party datasets and models, as well as implementing controls during procurement processes.

2. **Monitor and Document Risks**: Regularly monitor AI risks and benefits from third-party resources. Document incidents involving third-party GAI data and systems to identify over-reliance on third-party data and establish fallbacks.

3. **Reassess Risks After Fine-Tuning**: After fine-tuning third-party GAI models, reassess the associated risks. This includes evaluating potential vulnerabilities such as data poisoning, malware, and compliance with data privacy and localization regulations.

4. **Establish Clear Guidelines**: Develop clear guidelines for transparency and risk management regarding the collection and use of third-party data for model inputs. This is crucial to mitigate intellectual property, data privacy, and information security risks.

5. **Implement Enhanced Processes**: Create enhanced processes for interacting with external GAI technologies or service providers. This may involve varying risk controls for different types of models (foundation models, fine-tuned models, and embedded tools).

6. **Contingency Processes**: Ensure that contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be high-risk.

By following these strategies, organizations can better manage the risks associated with GAI and third-party components, as well as mitigate potential human misuse.","The suggested actions to manage risks of GAI with third-party components and human misuse involve targeting risks unique to or exacerbated by GAI. These actions include implementing AI risk management activities and actions set forth in the AI RMF 1.0 and Playbook, applying activities suggested in the AI RMF and its Playbook, and tailoring risk management resources relative to the severity and likelihood of negative impacts. Organizations are encouraged to identify and manage GAI risks based on the characteristics of the GAI model, system, or use case at hand, including GAI model or system architecture, training mechanisms and libraries, data types used for training or fine-tuning, levels of model access or availability of model weights, and application or use case context. Mitigations for model or system level risks may differ from mitigations for use-case or ecosystem level risks. Some GAI risks are unknown, making it difficult to properly scope or evaluate them, while others may be known but difficult to estimate due to the wide range of GAI stakeholders, uses, inputs, and outputs. Challenges with risk estimation are further complicated by a lack of visibility into GAI training data and the generally immature state of the science of AI measurement and safety today. Future updates may incorporate additional risks or provide further details on the risks identified. To guide organizations in identifying and managing GAI risks, a set of risks unique to or exacerbated by the development and use of GAI are defined, each labeled according to the outcome, object, or source of the risk. These risks provide a lens through which organizations can frame and execute risk management efforts, mapped to relevant Trustworthy AI Characteristics identified in the AI RMF.",0.9523809523809523,0.9057375712299506,0.5714285714285714,0.999999999975,0.6645228538654018
How can stakeholder feedback improve GAI system outcomes and reduce risks?,"['its potential ecosystem-level or longitudinal risks and related political, social, and economic impacts. \nGaps between benchmarks and real-world use of GAI systems may likely be exacerbated due to prompt \nsensitivity and broad heterogeneity of contexts of use. \nA.1.5. Structured Public Feedback \nStructured public feedback can be used to evaluate whether GAI systems are performing as intended \nand to calibrate and verify traditional measurement methods. Examples of structured feedback include, \nbut are not limited to: \n• \nParticipatory Engagement Methods: Methods used to solicit feedback from civil society groups, \naﬀected communities, and users, including focus groups, small user studies, and surveys. \n•'
 '20 \nGV-4.3-003 \nVerify information sharing and feedback mechanisms among individuals and \norganizations regarding any negative impact from GAI systems. \nInformation Integrity; Data \nPrivacy \nAI Actor Tasks: AI Impact Assessment, Aﬀected Individuals and Communities, Governance and Oversight \n \nGOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those \nexternal to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI \nrisks. \nAction ID \nSuggested Action \nGAI Risks \nGV-5.1-001 \nAllocate time and resources for outreach, feedback, and recourse processes in GAI \nsystem development.'
 'corresponding applications can help enhance awareness of performance changes and mitigate potential \nrisks and harms from outputs. There are many ways to capture and make use of user feedback – before \nand after GAI systems and digital content transparency approaches are deployed – to gain insights about \nauthentication eﬃcacy and vulnerabilities, impacts of adversarial threats on techniques, and unintended \nconsequences resulting from the utilization of content provenance approaches on users and \ncommunities. Furthermore, organizations can track and document the provenance of datasets to identify \ninstances in which AI-generated data is a potential root cause of performance issues with the GAI \nsystem. \nA.1.8. Incident Disclosure'
 'origins and modiﬁcations. Further narrowing of GAI task deﬁnitions to include provenance data can \nenable organizations to maximize the utility of provenance data and risk management eﬀorts. \nA.1.7. Enhancing Content Provenance through Structured Public Feedback \nWhile indirect feedback methods such as automated error collection systems are useful, they often lack \nthe context and depth that direct input from end users can provide. Organizations can leverage feedback \napproaches described in the Pre-Deployment Testing section to capture input from external sources such \nas through AI red-teaming.  \nIntegrating pre- and post-deployment external feedback into the monitoring process for GAI models and']","Stakeholder feedback can significantly improve GAI system outcomes and reduce risks in several ways:

1. **Evaluation of Performance**: Structured public feedback allows organizations to assess whether GAI systems are functioning as intended. This can help identify any discrepancies between benchmarks and real-world performance, which may be influenced by factors such as prompt sensitivity and the diverse contexts in which GAI systems are used.

2. **Participatory Engagement**: Engaging with civil society groups, affected communities, and users through methods like focus groups, small user studies, and surveys can provide valuable insights into the societal impacts of GAI systems. This participatory approach ensures that the voices of those potentially affected by the technology are heard and considered.

3. **Feedback Integration**: Organizations can establish policies and practices to collect, prioritize, and integrate feedback from external stakeholders regarding the potential individual and societal impacts of GAI systems. This can lead to more informed decision-making and risk management.

4. **Incident Disclosure and Transparency**: By capturing user feedback before and after the deployment of GAI systems, organizations can gain insights into performance changes, vulnerabilities, and unintended consequences. This transparency can enhance awareness of risks and help mitigate potential harms.

5. **Provenance Tracking**: Feedback can also aid in tracking and documenting the provenance of datasets, which can help identify instances where AI-generated data may contribute to performance issues. Understanding the origins and modifications of data can enhance risk management efforts.

Overall, integrating structured feedback from stakeholders into the development and monitoring processes of GAI systems can lead to improved outcomes and a reduction in associated risks.","Stakeholder feedback can improve GAI system outcomes and reduce risks by providing valuable insights into the quality and integrity of data used in training, the impact of AI-generated content, and the identification of harmful biases or homogenization. By seeking feedback from affected communities and engaging in structured feedback mechanisms like red-teaming, organizations can monitor and improve GAI outputs, enhance information integrity, and define use cases where human feedback exercises would be most beneficial for risk measurement and management. Additionally, involving internal experts, domain experts, users, and AI actors external to the development team in regular assessments and updates can help ensure a comprehensive evaluation of GAI risks and opportunities.",1.0,0.9999999999999988,1.0,0.999999999975,0.44110691775862393
How can automated system creators provide clear user notifications?,"['Provide a mechanism to conveniently opt out from automated systems in favor of a human \nalternative, where appropriate \nBrief, clear, accessible notice and instructions. Those impacted by an automated system should be \ngiven a brief, clear notice that they are entitled to opt-out, along with clear instructions for how to opt-out. \nInstructions should be provided in an accessible form and should be easily findable by those impacted by the \nautomated system. The brevity, clarity, and accessibility of the notice and instructions should be assessed (e.g., \nvia user experience research). \nHuman alternatives provided when appropriate. In many scenarios, there is a reasonable expectation'
 'You should know that an automated system is being used, \nand understand how and why it contributes to outcomes \nthat impact you. Designers, developers, and deployers of automat\xad\ned systems should provide generally accessible plain language docu\xad\nmentation including clear descriptions of the overall system func\xad\ntioning and the role automation plays, notice that such systems are in \nuse, the individual or organization responsible for the system, and ex\xad\nplanations of outcomes that are clear, timely, and accessible. Such \nnotice should be kept up-to-date and people impacted by the system \nshould be notified of significant use case or key functionality chang\xad\nes. You should know how and why an outcome impacting you was de\xad'
 'NOTICE & \nEXPLANATION \nWHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS\nThe expectations for automated systems are meant to serve as a blueprint for the development of additional \ntechnical standards and practices that are tailored for particular sectors and contexts. \nAn automated system should provide demonstrably clear, timely, understandable, and accessible notice of use, and \nexplanations as to how and why a decision was made or an action was taken by the system. These expectations are \nexplained below. \nProvide clear, timely, understandable, and accessible notice of use and explanations \xad\nGenerally accessible plain language documentation. The entity responsible for using the automated'
 'system functioning and the role automation plays, notice that such systems are in use, the individual or organiza\xad\ntion responsible for the system, and explanations of outcomes that are clear, timely, and accessible. Such notice \nshould be kept up-to-date and people impacted by the system should be notified of significant use case or key \nfunctionality changes. You should know how and why an outcome impacting you was determined by an \nautomated system, including when the automated system is not the sole input determining the outcome. \nAutomated systems should provide explanations that are technically valid, meaningful and useful to you and to']","Automated system creators can provide clear user notifications by following these guidelines:

1. **Accessible Documentation**: They should offer generally accessible plain language documentation that includes clear descriptions of the overall system functioning and the role automation plays.

2. **Notice of Use**: Users should be informed that an automated system is being used, along with the identity of the individual or organization responsible for the system.

3. **Timely Updates**: Notifications should be kept up-to-date, and users should be informed of significant use case or key functionality changes.

4. **Clear Explanations**: The system should provide clear, timely, understandable, and accessible explanations of how and why decisions were made or actions were taken by the system.

5. **User Experience Research**: The clarity, brevity, and accessibility of the notice and instructions should be assessed through user experience research to ensure they meet the needs of those impacted by the automated system.

By implementing these practices, automated system creators can enhance user understanding and trust in their systems.","Automated system creators can provide clear user notifications by ensuring that generally accessible plain language documentation is available, describing the overall system functioning, the role of automation, the responsible entity, and clear explanations of outcomes. These notifications should be kept up-to-date, timely, and accessible to users impacted by the system, with a focus on clarity and quality of information. Explanations should be technically valid, meaningful, and useful, tailored to the level of risk based on the context, and provided in a manner that is understandable to users and operators.",0.875,0.9676766545060191,1.0,0.999999999975,0.8881413089150522
What's needed for an algorithmic impact assessment for automated systems to ensure equity and accessibility?,"['consultation, design stage equity assessments (potentially including qualitative analysis), accessibility \ndesigns and testing, disparity testing, document any remaining disparities, and detail any mitigation \nimplementation and assessments. This algorithmic impact assessment should be made public whenever \npossible. Reporting should be provided in a clear and machine-readable manner using plain language to \nallow for more straightforward public accountability. \n28\nAlgorithmic \nDiscrimination \nProtections'
 'may violate legal protections. Designers, developers, and deployers of automated systems should take \nproactive \nand \ncontinuous \nmeasures \nto \nprotect \nindividuals \nand \ncommunities \nfrom algorithmic \ndiscrimination and to use and design systems in an equitable way. This protection should include proactive \nequity assessments as part of the system design, use of representative data and protection against proxies \nfor demographic features, ensuring accessibility for people with disabilities in design and development, \npre-deployment and ongoing disparity testing and mitigation, and clear organizational oversight. Independent \nevaluation and plain language reporting in the form of an algorithmic impact assessment, including'
 'and deployers of automated systems should take proactive and \ncontinuous measures to protect individuals and communities \nfrom algorithmic discrimination and to use and design systems in \nan equitable way.  This protection should include proactive equity \nassessments as part of the system design, use of representative data \nand protection against proxies for demographic features, ensuring \naccessibility for people with disabilities in design and development, \npre-deployment and ongoing disparity testing and mitigation, and \nclear organizational oversight. Independent evaluation and plain \nlanguage reporting in the form of an algorithmic impact assessment, \nincluding disparity testing results and mitigation information,'
 'enforcement or national security restrictions prevent doing so. Care should be taken to balance individual \nprivacy with evaluation data access needs; in many cases, policy-based and/or technological innovations and \ncontrols allow access to such data without compromising privacy. \nReporting. Entities responsible for the development or use of automated systems should provide \nreporting of an appropriately designed algorithmic impact assessment,50 with clear specification of who \nperforms the assessment, who evaluates the system, and how corrective actions are taken (if necessary) in \nresponse to the assessment. This algorithmic impact assessment should include at least: the results of any']","An algorithmic impact assessment for automated systems to ensure equity and accessibility should include the following elements:

1. **Proactive Equity Assessments**: These should be integrated as part of the system design process to identify and address potential disparities.

2. **Use of Representative Data**: Ensuring that the data used in the system is representative of the diverse populations it affects, and protecting against proxies for demographic features that could lead to discrimination.

3. **Accessibility Considerations**: The design and development of the system should ensure accessibility for people with disabilities.

4. **Disparity Testing**: Conducting pre-deployment and ongoing testing to identify and mitigate any disparities that may arise from the system's use.

5. **Clear Organizational Oversight**: Establishing oversight mechanisms to ensure accountability in the design and deployment of the system.

6. **Independent Evaluation**: Having an independent party evaluate the system to ensure objectivity in the assessment process.

7. **Plain Language Reporting**: The results of the algorithmic impact assessment should be reported in a clear and machine-readable manner, using plain language to facilitate public understanding and accountability.

8. **Documentation of Mitigation Efforts**: Detailing any remaining disparities and the steps taken to mitigate them, along with corrective actions if necessary.

These components are essential to protect individuals and communities from algorithmic discrimination and to promote equitable system design.",The answer to given question is not present in context,1.0,0.9778056083511073,1.0,0.0,0.17296358078174565
How can harmful bias in GAI systems be addressed while considering environmental impact and sustainability of AI training?,"['emissions for LLM inference, generative tasks (e.g., text summarization) were found to be more energy- \nand carbon-intensive than discriminative or non-generative tasks (e.g., text classiﬁcation).  \nMethods for creating smaller versions of trained models, such as model distillation or compression, \ncould reduce environmental impacts at inference time, but training and tuning such models may still \ncontribute to their environmental impacts. Currently there is no agreed upon method to estimate \nenvironmental impacts from GAI.  \nTrustworthy AI Characteristics: Accountable and Transparent, Safe \n2.6. Harmful Bias and Homogenization \nBias exists in many forms and can become ingrained in automated systems. AI systems, including GAI'
 'stereotypical content can also further exacerbate representational harms (see Harmful Bias and \nHomogenization below).  \nTrustworthy AI Characteristics: Safe, Secure and Resilient \n2.4. Data Privacy \nGAI systems raise several risks to privacy. GAI system training requires large volumes of data, which in \nsome cases may include personal data. The use of personal data for GAI training raises risks to widely \naccepted privacy principles, including to transparency, individual participation (including consent), and \npurpose speciﬁcation. For example, most model developers do not disclose speciﬁc data sources on \nwhich models were trained, limiting user awareness of whether personally identiﬁably information (PII)'
 'may stem from their training data, can also cause representational harms or perpetuate or exacerbate \nbias based on race, gender, disability, or other protected classes.  \nHarmful bias in GAI systems can also lead to harms via disparities between how a model performs for \ndiﬀerent subgroups or languages (e.g., an LLM may perform less well for non-English languages or \ncertain dialects). Such disparities can contribute to discriminatory decision-making or ampliﬁcation of \nexisting societal biases. In addition, GAI systems may be inappropriately trusted to perform similarly \nacross all subgroups, which could leave the groups facing underperformance with worse outcomes than'
 '37 \nMS-2.11-005 \nAssess the proportion of synthetic to non-synthetic training data and verify \ntraining data is not overly homogenous or GAI-produced to mitigate concerns of \nmodel collapse. \nHarmful Bias and Homogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Aﬀected Individuals and Communities, Domain Experts, End-Users, \nOperation and Monitoring, TEVV \n \nMEASURE 2.12: Environmental impact and sustainability of AI model training and management activities – as identiﬁed in the MAP \nfunction – are assessed and documented. \nAction ID \nSuggested Action \nGAI Risks \nMS-2.12-001 Assess safety to physical environments when deploying GAI systems. \nDangerous, Violent, or Hateful \nContent']","Harmful bias in Generative AI (GAI) systems can be addressed through several strategies that also consider the environmental impact and sustainability of AI training:

1. **Diverse and Representative Training Data**: To mitigate harmful bias, it is essential to assess the proportion of synthetic to non-synthetic training data and ensure that the training data is not overly homogeneous or predominantly GAI-produced. This can help prevent model collapse and ensure that the model performs well across different subgroups and languages.

2. **Model Distillation and Compression**: Creating smaller versions of trained models through methods like model distillation or compression can reduce the environmental impacts during inference. While these methods may still contribute to environmental impacts during training and tuning, they can help in deploying models that are more efficient and less resource-intensive.

3. **Environmental Impact Assessment**: It is important to assess and document the environmental impact and sustainability of AI model training and management activities. This includes evaluating the energy consumption and carbon emissions associated with training large models, which can inform better practices and reduce overall environmental harm.

4. **Transparency and Accountability**: Ensuring transparency in the data sources used for training GAI systems can help users understand potential biases and the origins of the data. This aligns with privacy principles and can lead to more accountable AI systems.

5. **Monitoring and Evaluation**: Continuous monitoring and evaluation of GAI systems can help identify and address biases as they arise. This includes assessing how models perform across different demographics and ensuring that any disparities are addressed promptly.

By integrating these strategies, it is possible to work towards reducing harmful bias in GAI systems while also considering their environmental impact and sustainability.","Harmful bias in GAI systems can be addressed by assessing the proportion of synthetic to non-synthetic training data, ensuring that training data is not overly homogenous or GAI-produced to mitigate concerns of model collapse. Additionally, it is important to document and assess the environmental impact and sustainability of AI model training and management activities, including measuring or estimating environmental impacts such as energy and water consumption for training, fine-tuning, and deploying models. Verification of the effectiveness of carbon capture or offset programs for GAI training and applications is also crucial to address green-washing concerns.",1.0,0.9494945939007483,0.6666666666666666,0.8055555555287036,0.7060129005054149
What's the NIST AI Risk Management Framework's purpose and how does it relate to other government agencies' AI ethical frameworks?,"['https://www.nist.gov/itl/ai-risk-management-framework \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 3: AI \nRisks and Trustworthiness. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/3-sec-characteristics \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Chapter 6: AI \nRMF Proﬁles. https://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Core_And_Proﬁles/6-sec-proﬁle \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix A: \nDescriptions of AI Actor Tasks. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_A#:~:text=AI%20actors%'
 '57 \nNational Institute of Standards and Technology (2023) AI Risk Management Framework, Appendix B: \nHow AI Risks Diﬀer from Traditional Software Risks. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Appendices/Appendix_B \nNational Institute of Standards and Technology (2023) AI RMF Playbook. \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook \nNational Institue of Standards and Technology (2023) Framing Risk \nhttps://airc.nist.gov/AI_RMF_Knowledge_Base/AI_RMF/Foundational_Information/1-sec-risk \nNational Institute of Standards and Technology (2023) The Language of Trustworthy AI: An In-Depth \nGlossary of Terms https://airc.nist.gov/AI_RMF_Knowledge_Base/Glossary'
 'NIST Trustworthy and Responsible AI  \nNIST AI 600-1 \nArtificial Intelligence Risk Management \nFramework: Generative Artificial \nIntelligence Profile \n \n \n \nThis publication is available free of charge from: \nhttps://doi.org/10.6028/NIST.AI.600-1 \n \nJuly 2024 \n \n \n \n \nU.S. Department of Commerce  \nGina M. Raimondo, Secretary \nNational Institute of Standards and Technology  \nLaurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology'
 'evaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-\ndriven, open, transparent, and collaborative process that includes workshops and other opportunities to provide \ninput. The NIST framework aims to foster the development of innovative approaches to address \ncharacteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy, \nrobustness, safety, security (resilience), and mitigation of unintended and/or harmful bias, as well as of \nharmful \nuses. \nThe \nNIST \nframework \nwill \nconsider \nand \nencompass \nprinciples \nsuch \nas \ntransparency, accountability, and fairness during pre-design, design and development, deployment, use,']","The NIST AI Risk Management Framework aims to foster the development of innovative approaches to address characteristics of trustworthiness in AI, including accuracy, explainability, interpretability, reliability, privacy, robustness, safety, security (resilience), and the mitigation of unintended and/or harmful bias, as well as harmful uses. It is developed through a consensus-driven, open, transparent, and collaborative process that includes workshops and opportunities for public input.

While the context does not provide specific details on how the NIST framework relates to other government agencies' AI ethical frameworks, it can be inferred that the NIST framework likely aligns with broader governmental efforts to ensure ethical AI practices, which may include principles such as transparency, accountability, and fairness. These principles are common themes in various AI ethical frameworks across different agencies. However, for a detailed comparison or relationship, additional information would be needed.","The NIST AI Risk Management Framework is being developed to better manage risks posed by AI to individuals, organizations, and society. It aims to help incorporate trustworthiness considerations into the design, development, use, and evaluation of AI products, services, and systems. The framework is intended to address characteristics of trustworthiness such as accuracy, explainability, reliability, privacy, robustness, safety, security, and mitigation of unintended bias. It also aims to consider principles like transparency, accountability, and fairness throughout the AI technology lifecycle. The NIST framework is expected to be released in the winter of 2022-23. Several U.S. government agencies have developed specific ethical frameworks for AI use. For example, the Department of Energy has the AI Advancement Council, the Department of Defense has adopted AI Ethical Principles, and the U.S. Intelligence Community has developed Principles of Artificial Intelligence Ethics and an AI Ethics Framework. These frameworks guide the ethical use and development of AI systems in various contexts.",1.0,0.9433726351893186,0.625,0.999999999975,0.6526941322870027
How can structured public feedback improve content provenance and AI incident risk?,"['corresponding applications can help enhance awareness of performance changes and mitigate potential \nrisks and harms from outputs. There are many ways to capture and make use of user feedback – before \nand after GAI systems and digital content transparency approaches are deployed – to gain insights about \nauthentication eﬃcacy and vulnerabilities, impacts of adversarial threats on techniques, and unintended \nconsequences resulting from the utilization of content provenance approaches on users and \ncommunities. Furthermore, organizations can track and document the provenance of datasets to identify \ninstances in which AI-generated data is a potential root cause of performance issues with the GAI \nsystem. \nA.1.8. Incident Disclosure'
 'origins and modiﬁcations. Further narrowing of GAI task deﬁnitions to include provenance data can \nenable organizations to maximize the utility of provenance data and risk management eﬀorts. \nA.1.7. Enhancing Content Provenance through Structured Public Feedback \nWhile indirect feedback methods such as automated error collection systems are useful, they often lack \nthe context and depth that direct input from end users can provide. Organizations can leverage feedback \napproaches described in the Pre-Deployment Testing section to capture input from external sources such \nas through AI red-teaming.  \nIntegrating pre- and post-deployment external feedback into the monitoring process for GAI models and'
 'distinguish human-generated content from AI-generated synthetic content. To help manage and mitigate \nthese risks, digital transparency mechanisms like provenance data tracking can trace the origin and \nhistory of content. Provenance data tracking and synthetic content detection can help facilitate greater \ninformation access about both authentic and synthetic content to users, enabling better knowledge of \ntrustworthiness in AI systems. When combined with other organizational accountability mechanisms, \ndigital content transparency approaches can enable processes to trace negative outcomes back to their \nsource, improve information integrity, and uphold public trust. Provenance data tracking and synthetic'
 '41 \nMG-2.2-006 \nUse feedback from internal and external AI Actors, users, individuals, and \ncommunities, to assess impact of AI-generated content. \nHuman-AI Conﬁguration \nMG-2.2-007 \nUse real-time auditing tools where they can be demonstrated to aid in the \ntracking and validation of the lineage and authenticity of AI-generated data. \nInformation Integrity \nMG-2.2-008 \nUse structured feedback mechanisms to solicit and capture user input about AI-\ngenerated content to detect subtle shifts in quality or alignment with \ncommunity and societal values. \nHuman-AI Conﬁguration; Harmful \nBias and Homogenization \nMG-2.2-009 \nConsider opportunities to responsibly use synthetic data and other privacy']","Structured public feedback can improve content provenance and AI incident risk in several ways:

1. **Enhanced Context and Depth**: Direct input from end users provides valuable context that automated error collection systems may lack. This feedback can help organizations understand the nuances of how AI-generated content is perceived and used, leading to better risk management.

2. **Integration into Monitoring Processes**: By incorporating pre- and post-deployment external feedback into the monitoring of Generative AI (GAI) models, organizations can gain insights into the effectiveness and vulnerabilities of their systems. This can help identify potential risks and harms associated with AI outputs.

3. **Provenance Data Utilization**: Narrowing GAI task definitions to include provenance data allows organizations to maximize the utility of this data. By tracking the origin and history of content, organizations can better assess the authenticity and lineage of AI-generated data, which is crucial for incident risk management.

4. **Detection of Quality Shifts**: Structured feedback mechanisms can help detect subtle shifts in the quality of AI-generated content and its alignment with community and societal values. This can alert organizations to potential issues before they escalate into significant risks.

5. **Accountability and Transparency**: When combined with digital transparency mechanisms, structured feedback can facilitate processes that trace negative outcomes back to their source. This enhances organizational accountability and helps uphold public trust in AI systems.

Overall, structured public feedback serves as a critical tool for improving content provenance and mitigating AI incident risks by fostering a more informed and responsive approach to AI system management.","Structured public feedback can enhance content provenance and AI incident risk management by providing direct input from end users, capturing insights about authentication efficacy, vulnerabilities, impacts of adversarial threats, and unintended consequences resulting from the utilization of content provenance approaches. Integrating pre- and post-deployment external feedback into the monitoring process for GAI models and applications can help enhance awareness of performance changes and mitigate potential risks and harms from outputs. Organizations can track and document the provenance of datasets to identify instances where AI-generated data may be a root cause of performance issues with the GAI system.",0.8,1.0,1.0,0.999999999975,0.854211117934947
How can GAI systems impact misinformation and cyber security risks?,"['10 \nGAI systems can ease the unintentional production or dissemination of false, inaccurate, or misleading \ncontent (misinformation) at scale, particularly if the content stems from confabulations.  \nGAI systems can also ease the deliberate production or dissemination of false or misleading information \n(disinformation) at scale, where an actor has the explicit intent to deceive or cause harm to others. Even \nvery subtle changes to text or images can manipulate human and machine perception. \nSimilarly, GAI systems could enable a higher degree of sophistication for malicious actors to produce \ndisinformation that is targeted towards speciﬁc demographics. Current and emerging multimodal models'
 'lowering the barriers for or easing automated exercise of oﬀensive capabilities; simultaneously, it \nexpands the available attack surface, as GAI itself is vulnerable to attacks like prompt injection or data \npoisoning.  \nOﬀensive cyber capabilities advanced by GAI systems may augment cybersecurity attacks such as \nhacking, malware, and phishing. Reports have indicated that LLMs are already able to discover some \nvulnerabilities in systems (hardware, software, data) and write code to exploit them. Sophisticated threat \nactors might further these risks by developing GAI-powered security co-pilots for use in several parts of \nthe attack chain, including informing attackers on how to proactively evade threat detection and escalate'
 'campaigns, which may not be photorealistic, but could enable these campaigns to gain more reach and \nengagement on social media platforms. Additionally, generative AI models can assist malicious actors in \ncreating fraudulent content intended to impersonate others. \nTrustworthy AI Characteristics: Accountable and Transparent, Safe, Valid and Reliable, Interpretable and \nExplainable \n2.9. Information Security \nInformation security for computer systems and data is a mature ﬁeld with widely accepted and \nstandardized practices for oﬀensive and defensive cyber capabilities. GAI-based systems present two \nprimary information security risks: GAI could potentially discover or enable new cybersecurity risks by'
 'over-rely on GAI systems or may unjustiﬁably perceive GAI content to be of higher quality than that \nproduced by other sources. This phenomenon is an example of automation bias, or excessive deference \nto automated systems. Automation bias can exacerbate other risks of GAI, such as risks of confabulation \nor risks of bias or homogenization. \nThere may also be concerns about emotional entanglement between humans and GAI systems, which \ncould lead to negative psychological impacts. \nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with \nHarmful Bias Managed, Privacy Enhanced, Safe, Valid and Reliable \n2.8. Information Integrity']","GAI (Generative AI) systems can significantly impact misinformation and cybersecurity risks in several ways:

1. **Misinformation Production**: GAI systems can facilitate the unintentional production or dissemination of false, inaccurate, or misleading content (misinformation), particularly when the content arises from confabulations. They can also enable the deliberate creation of false or misleading information (disinformation) by actors with the intent to deceive or cause harm. Subtle alterations to text or images generated by GAI can manipulate both human and machine perception, making it easier to spread misinformation.

2. **Targeted Disinformation Campaigns**: GAI systems may allow malicious actors to create sophisticated disinformation campaigns that are tailored to specific demographics, increasing the effectiveness and reach of such campaigns on social media platforms.

3. **Cybersecurity Risks**: GAI systems can lower the barriers for automated offensive capabilities, expanding the attack surface for cyber threats. They are vulnerable to attacks like prompt injection or data poisoning, which can be exploited by threat actors. Reports indicate that GAI models can discover vulnerabilities in systems and write code to exploit them, potentially augmenting cybersecurity attacks such as hacking, malware, and phishing.

4. **Development of GAI-Powered Attack Tools**: Sophisticated threat actors might develop GAI-powered security co-pilots that assist in various stages of cyber attacks, including evading threat detection and escalating attacks.

Overall, while GAI systems have the potential to enhance various applications, they also pose significant risks related to misinformation and cybersecurity that need to be managed carefully.","GAI systems can ease the unintentional production or dissemination of false, inaccurate, or misleading content (misinformation) at scale, particularly if the content stems from confabulations. GAI systems can also ease the deliberate production or dissemination of false or misleading information (disinformation) at scale, where an actor has the explicit intent to deceive or cause harm to others. Even very subtle changes to text or images can manipulate human and machine perception. Similarly, GAI systems could enable a higher degree of sophistication for malicious actors to produce disinformation that is targeted towards specific demographics. Current and emerging multimodal models make it possible to generate both text-based disinformation and highly realistic “deepfakes” – that is, synthetic audiovisual content and photorealistic images. Additional disinformation threats could be enabled by future GAI models trained on new data modalities. Disinformation and misinformation – both of which may be facilitated by GAI – may erode public trust in true or valid evidence and information, with downstream effects. For example, a synthetic image of a Pentagon blast went viral and briefly caused a drop in the stock market. Generative AI models can also assist malicious actors in creating compelling imagery and propaganda to support disinformation campaigns, which may not be photorealistic, but could enable these campaigns to gain more reach and engagement on social media platforms. Additionally, generative AI models can assist malicious actors in creating fraudulent content intended to impersonate others.",0.9047619047619048,0.9740948582893388,0.6,0.999999999975,0.7754067452216589
"What steps are needed in automated systems for public safety, including consultation, testing, and risk identification?","['the system, and other concerns identified via the consultation process. Assessment and, where possible, mea\xad\nsurement of the impact of risks should be included and balanced such that high impact risks receive attention \nand mitigation proportionate with those impacts. Automated systems with the intended purpose of violating \nthe safety of others should not be developed or used; systems with such safety violations as identified unin\xad\ntended consequences should not be used until the risk can be mitigated. Ongoing risk mitigation may necessi\xad\ntate rollback or significant modification to a launched automated system. \n18'
 'Consultation. The public should be consulted in the design, implementation, deployment, acquisition, and \nmaintenance phases of automated system development, with emphasis on early-stage consultation before a \nsystem is introduced or a large change implemented. This consultation should directly engage diverse impact\xad\ned communities to consider concerns and risks that may be unique to those communities, or disproportionate\xad\nly prevalent or severe for them. The extent of this engagement and the form of outreach to relevant stakehold\xad\ners may differ depending on the specific automated system and development phase, but should include \nsubject matter, sector-specific, and context-specific experts as well as experts on potential impacts such as'
 'civil rights, civil liberties, and privacy experts. For private sector applications, consultations before product \nlaunch may need to be confidential. Government applications, particularly law enforcement applications or \napplications that raise national security considerations, may require confidential or limited engagement based \non system sensitivities and preexisting oversight laws and structures. Concerns raised in this consultation \nshould be documented, and the automated system developers were proposing to create, use, or deploy should \nbe reconsidered based on this feedback. \nTesting. Systems should undergo extensive testing before deployment. This testing should follow'
 'SAFE AND EFFECTIVE SYSTEMS \nYou should be protected from unsafe or ineffective sys\xad\ntems. Automated systems should be developed with consultation \nfrom diverse communities, stakeholders, and domain experts to iden\xad\ntify concerns, risks, and potential impacts of the system. Systems \nshould undergo pre-deployment testing, risk identification and miti\xad\ngation, and ongoing monitoring that demonstrate they are safe and \neffective based on their intended use, mitigation of unsafe outcomes \nincluding those beyond the intended use, and adherence to do\xad\nmain-specific standards. Outcomes of these protective measures \nshould include the possibility of not deploying the system or remov\xad']","To ensure public safety in automated systems, the following steps are needed:

1. **Consultation**: The public should be consulted during the design, implementation, deployment, acquisition, and maintenance phases of automated system development. This consultation should engage diverse impacted communities to address unique concerns and risks. The engagement should include subject matter experts, sector-specific experts, and those knowledgeable about civil rights, civil liberties, and privacy.

2. **Risk Identification and Mitigation**: Automated systems should be developed with input from various stakeholders to identify potential risks and impacts. High-impact risks should receive appropriate attention and mitigation efforts. Systems intended to violate safety should not be developed or used, and any unintended safety violations should be addressed before deployment.

3. **Testing**: Extensive testing of the systems should be conducted before deployment. This testing should include risk identification and mitigation strategies to ensure the systems are safe and effective for their intended use.

4. **Ongoing Monitoring**: After deployment, there should be ongoing monitoring to ensure the systems continue to operate safely and effectively, adhering to domain-specific standards.

5. **Documentation and Reconsideration**: Concerns raised during the consultation process should be documented, and the development of the automated system should be reconsidered based on this feedback.

These steps aim to protect individuals from unsafe or ineffective automated systems.","The steps needed in automated systems for public safety include consultation with the public during design, implementation, deployment, acquisition, and maintenance phases, emphasizing early-stage consultation; extensive testing before deployment following domain-specific best practices; and proactive and ongoing risk identification and mitigation to address potential risks that could impact people's rights, opportunities, or access. This includes identifying risks from purposeful misuse of the system and other concerns raised during the consultation process.",1.0,0.9242137730953525,1.0,0.999999999975,0.7378084806548086
How does GAI risk management address information integrity and prevent illegal content generation?,"[""GAI Risks \nMP-5.1-001 Apply TEVV practices for content provenance (e.g., probing a system's synthetic \ndata generation capabilities for potential misuse or vulnerabilities. \nInformation Integrity; Information \nSecurity \nMP-5.1-002 \nIdentify potential content provenance harms of GAI, such as misinformation or \ndisinformation, deepfakes, including NCII, or tampered content. Enumerate and \nrank risks based on their likelihood and potential impact, and determine how well \nprovenance solutions address speciﬁc risks and/or harms. \nInformation Integrity; Dangerous, \nViolent, or Hateful Content; \nObscene, Degrading, and/or \nAbusive Content \nMP-5.1-003 \nConsider disclosing use of GAI to end users in relevant contexts, while considering""
 'GOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other \ncontrols based on organizational risk priorities. \nAction ID \nSuggested Action \nGAI Risks \nGV-1.4-001 \nEstablish policies and mechanisms to prevent GAI systems from generating \nCSAM, NCII or content that violates the law.  \nObscene, Degrading, and/or \nAbusive Content; Harmful Bias \nand Homogenization; \nDangerous, Violent, or Hateful \nContent \nGV-1.4-002 \nEstablish transparent acceptable use policies for GAI that address illegal use or \napplications of GAI. \nCBRN Information or \nCapabilities; Obscene, \nDegrading, and/or Abusive \nContent; Data Privacy; Civil \nRights violations'
 'Abusive Content; Harmful Bias and \nHomogenization; Dangerous, \nViolent, or Hateful Content \nMG-2.2-002 \nDocument training data sources to trace the origin and provenance of AI-\ngenerated content. \nInformation Integrity \nMG-2.2-003 \nEvaluate feedback loops between GAI system content provenance and human \nreviewers, and update where needed. Implement real-time monitoring systems \nto aﬃrm that content provenance protocols remain eﬀective.  \nInformation Integrity \nMG-2.2-004 \nEvaluate GAI content and data for representational biases and employ \ntechniques such as re-sampling, re-ranking, or adversarial training to mitigate \nbiases in the generated content. \nInformation Security; Harmful Bias \nand Homogenization \nMG-2.2-005'
 'enhancing techniques in GAI development, where appropriate and applicable, \nmatch the statistical properties of real-world data without disclosing personally \nidentiﬁable information or contributing to homogenization. \nData Privacy; Intellectual Property; \nInformation Integrity; \nConfabulation; Harmful Bias and \nHomogenization \nAI Actor Tasks: AI Deployment, AI Impact Assessment, Governance and Oversight, Operation and Monitoring \n \nMANAGE 2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identiﬁed. \nAction ID \nSuggested Action \nGAI Risks \nMG-2.3-001 \nDevelop and update GAI system incident response and recovery plans and']","GAI risk management addresses information integrity and prevents illegal content generation through several suggested actions:

1. **TEVV Practices for Content Provenance**: It emphasizes applying TEVV (Testing, Evaluation, Validation, and Verification) practices to probe a system's synthetic data generation capabilities for potential misuse or vulnerabilities, which helps ensure the integrity of the information generated.

2. **Identifying Potential Harms**: The management process includes identifying potential content provenance harms, such as misinformation, disinformation, deepfakes, and tampered content. By enumerating and ranking these risks based on their likelihood and potential impact, organizations can better understand and mitigate risks to information integrity.

3. **Policies to Prevent Illegal Content**: Establishing policies and mechanisms to prevent GAI systems from generating illegal content, such as Child Sexual Abuse Material (CSAM) and Non-Consensual Intimate Images (NCII), is crucial. This includes creating transparent acceptable use policies that address illegal applications of GAI.

4. **Documenting Training Data Sources**: Documenting the sources of training data helps trace the origin and provenance of AI-generated content, which is essential for maintaining information integrity.

5. **Evaluating Feedback Loops**: The management process involves evaluating feedback loops between GAI system content provenance and human reviewers, ensuring that content provenance protocols remain effective through real-time monitoring.

6. **Mitigating Representational Biases**: Evaluating GAI content for representational biases and employing techniques such as re-sampling, re-ranking, or adversarial training helps to mitigate biases in generated content, thereby supporting information integrity.

These actions collectively contribute to a framework that aims to uphold information integrity while preventing the generation of illegal content.","GAI risk management addresses information integrity by obtaining input from stakeholder communities to identify unacceptable use, maintaining an updated hierarchy of identified and expected GAI risks, reevaluating organizational risk tolerances, and devising a plan to halt development or deployment of a GAI system that poses unacceptable negative risk. This process helps prevent illegal content generation by establishing policies and mechanisms to prevent GAI systems from generating illegal content, as well as transparent acceptable use policies that address illegal use or applications of GAI.",0.8888888888888888,0.9846446622238562,0.3333333333333333,0.999999999975,0.3398176086930595
