{"questions": {"83094a08-78e8-4ba7-acd6-7e180f1e77b9": "What is the title of the publication related to Artificial Intelligence Risk Management by NIST?", "7b97d512-0aa6-4b2c-aab4-c9a8a6f51a01": "Where can the NIST AI 600-1 publication be accessed for free?", "c31695f4-3344-41cc-b137-1a9f0d95512a": "What is the title of the publication released by NIST in July 2024 regarding AI risk management?", "39919ba6-319e-419b-9fb0-4a7281db782f": "Who is the Secretary of the U.S. Department of Commerce mentioned in the context?", "10c4f2c6-ddc9-4f96-a506-9190fe8f73e8": "What is the primary goal of NIST in relation to artificial intelligence?", "93fe07e5-d45e-4176-9f72-ed2145e6b532": "How long has NIST been conducting work on AI?", "26fd8863-3c34-4c79-9ae6-79c9fb954f34": "What contributions did the NIST Generative AI Public Working Group make to the report on trustworthy development and use of AI?", "79c3b833-26fc-4d85-9b6a-c441cbf5800b": "Who are some of the individuals acknowledged for their helpful comments and contributions in the report?", "f1425260-20bc-4de7-b6c3-1c21778d5ddb": "What is the address of the National Institute of Standards and Technology (NIST) mentioned in the context?", "f9734ff1-9752-4725-9b62-a6bf171a2092": "Where can additional information about NIST AI publications be found?", "ad181ee2-5580-4b2e-ae85-55e50eb6e134": "What is the purpose of the information provided in the context?", "945ba2fc-e79c-4f1d-9b91-95f4231f3877": "Does the context suggest any endorsement or recommendation by a U.S. Government agency?", "280a4aba-cc51-4d98-80d7-a66644a61369": "What is the focus of the section titled \"Overview of Risks Unique to or Exacerbated by GAI\"?", "f14a6c3e-1970-45e2-b873-73d45cc23e53": "Where can one find the suggested actions to manage GAI risks in the document?", "ea6ad41c-e88b-4220-bcb4-7188e8778f9a": "What is the purpose of the AI Risk Management Framework (AI RMF 1.0) for Generative AI?", "18e6d086-2e28-441f-bd2d-0d40756a1940": "How does the AI RMF aim to improve organizations' ability to incorporate trustworthiness considerations?", "89a1cc96-6404-4f21-803a-ae396c251bc5": "What are the main purposes of AI RMF profiles in managing AI risks for organizations?", "985eef43-2d4a-41e4-b645-a77cc7ef933e": "How does the cross-sectoral profile of GAI assist in addressing risks across different use cases or sectors?", "5b2cfb49-5af8-4a83-a99f-9ec77948f2b4": "What are the risks associated with the use of Generative AI (GAI) as defined in the document?", "2ebea0dc-d484-4320-bb48-3e458e2bee2a": "How does the document suggest organizations manage the risks related to large language models (LLMs) and cloud-based services?", "7295441c-8a71-4414-bc3d-39d923cc2b8e": "What is the definition of \"dual-use foundation models\" as per EO 14110?", "4b66d747-273c-45d5-a23c-e4c3997d30b8": "What directive does Section 4.1(a)(i)(A) of EO 14110 give to the Secretary of Commerce regarding generative AI?", "7ef08335-de05-4c8a-b05a-a43468d8a7d4": "What were the four primary considerations relevant to Generative AI that the GAI PWG focused on?", "2da6b6b2-44f0-4e30-8acb-746ad6ff3243": "How did the GAI PWG obtain multistakeholder input on GAI risk management?", "d258ad89-cc1d-408d-bd61-e383bd68d9d6": "What will future revisions of the profile include regarding AI RMF subcategories and risks?", "7f330995-62d9-4b2f-bcbb-5d7c524faf02": "Where will the glossary of terms related to GAI risk management be hosted?", "6b51c17c-b2db-4c5f-9316-46e407628952": "What factors can influence the likelihood and magnitude of risks in a given context?", "70768f6b-ce11-484f-8f03-564ca97508c3": "How do GAI risks differ from traditional software risks?", "e883cffb-b753-4281-8408-e5c36dc5d05c": "What are the different levels at which risks may exist in the context of GAI models and systems?", "6005b6e6-1cf5-45cd-9dc1-ee20ad69eb67": "How can human behavior contribute to the risks associated with GAI models?", "e9e5a946-c25c-4ef7-ad85-7c4dfeed0baa": "What are the potential risks associated with \"algorithmic monocultures\" in decision-making settings?", "a11db022-ada0-445c-8a76-0a4de548558d": "How has the impact of Generative AI (GAI) on the labor market been studied compared to traditional AI?", "139ce322-30c2-4fb7-b216-fea6af1ddb90": "What are some examples of immediate risks associated with the misuse of AI systems?", "3c44db7f-2a21-4d50-9bc2-a51bce531958": "How do the characteristics of a GAI model influence the presence and type of risks it may pose?", "84eed875-d5c0-4d53-beb3-6dce971a018e": "What factors should organizations consider when tailoring their measurement of GAI risks based on model architecture and training mechanisms?", "29849ab7-4c37-445d-a838-0d31178b9af5": "How do the levels of model access and availability of model weights influence the allocation of risk management resources in GAI applications?", "e8385bd9-7530-4acf-8f9a-9c6caab66018": "What are some challenges associated with estimating risks related to GAI?", "c817d4a9-767b-420c-a189-658ba5039699": "Why are speculative risks not considered in the current document on GAI risks?", "bde37c09-63ee-414f-9ef9-0ff88b6bd024": "What are the unique risks associated with the development and use of GAI that organizations need to identify and manage?", "053fd158-e84d-4b66-abfc-03daab5397bd": "How are the identified risks related to Trustworthy AI Characteristics in the AI RMF?", "27a3e889-f524-49b1-a2a0-05e645358e39": "What are the two main categories of risks associated with advanced AI as mentioned in the context?", "f8e2935f-305d-4bfb-bdee-f74e384a6329": "Can you list some examples of technical/model risks identified in the context?", "538baa87-b1a3-4ecc-85d0-683a5766d49f": "What are the different categories of ecosystem or societal risks mentioned in the context?", "9733a6fc-abed-4d4c-80af-488a4a2fc61e": "How are some risks described in the context as being cross-cutting between the categories?", "b228ced9-37d8-4464-8c22-9d09ed3cfee2": "What are the potential risks associated with eased access to CBRN information or capabilities?", "4af91004-5810-4afc-9979-7b1f732049cb": "How does confabulation contribute to the spread of misinformation among users?", "08c3137e-14a4-49a2-9eeb-917a4debbb9c": "What are the potential consequences of data privacy breaches involving biometric or health information?", "5e658e0f-d0c4-4e87-b97a-793402c1c23f": "How can the environmental impacts of training or operating GAI models affect ecosystems?", "9cbcdba2-b7c6-4cba-8926-32e999f4a3dc": "What are some potential consequences of erroneous outputs from AI systems on decision-making?", "bf5e61ed-e3fe-4a5e-ae1e-5228c57102e1": "How can human-AI configuration lead to emotional entanglement with GAI systems?", "253a921c-6f61-4a84-bb39-a8a259376547": "What are some examples of sensitive information that can be categorized as sensitive PII?", "8dbc8cee-dfab-4302-ad64-eabfb142f2da": "How do lowered barriers for offensive cyber capabilities impact the ease of hacking and exploitation of vulnerabilities?", "76978900-a269-49f0-b2ca-63e64c78c4c8": "What challenges arise in establishing a baseline scenario when assessing harm caused by disparities between groups in AI behaviors?", "5c529f04-dc09-4aae-ba5c-554650b5d0ea": "How does the presence of an AI system influence the determination of harm in situations where biased behavior affects different subgroups?", "fb529026-96d6-4f2b-9fea-8dc8dbce4740": "What are the potential risks associated with increased attack surfaces for targeted cyberattacks?", "371c096e-e2a9-4b14-9103-cdcfcf685433": "How might the easing of production or replication of copyrighted content impact intellectual property rights?", "fc3f78a5-02a3-402f-9299-c279a17eaac3": "What are the potential risks associated with the non-transparent integration of upstream third-party components in the context of GAI?", "6319e3d3-5a7f-439c-9c2d-f7dab57dc44b": "How might GAI facilitate access to CBRN weapons or related knowledge for malicious actors?", "d1cda133-bdca-4ba3-afab-f3d396f6450b": "How do LLMs compare to traditional search engines in providing assistance for biological threat creation and attack planning?", "1fec0800-e2ee-4de8-b5e7-7b15ce37bc67": "What factors are necessary for the physical synthesis, development, production, and use of chemical or biological agents?", "9bf1b526-dd7e-46fc-b982-69e7f6e3bf41": "What are the key barriers for malicious actors in the misuse of chemical or biological agents as discussed in the context?", "42dffb3e-7170-4bd7-b8ac-f5f02824878c": "How might chemical and biological design tools (BDTs) enhance design capabilities in chemistry and biology compared to text-based LLMs?", "5b988b3e-4836-4c67-bc4d-e7900fedcb2e": "What are the ongoing assessments mentioned in the context aimed at monitoring regarding AI tools and CBRN weapons planning?", "1b210d31-e962-4c3d-befc-27c4874921a3": "What are the three characteristics of trustworthy AI highlighted in the context?", "5e2b3170-1271-4bbe-a769-914446607627": "What does the term \"confabulation\" refer to in the context of GAI systems?", "9a454af9-b0a7-491f-8990-75a6102fc79f": "How do confabulations relate to the design of generative models like LLMs?", "9a5844ef-c9ee-4f74-b8fc-9b11a83a9755": "What are the potential risks associated with confabulations in statistical predictions, particularly in healthcare?", "fd2a9cbc-3d1b-431a-8e4a-7f8f3950093b": "How can the confident nature of a response contribute to the spread of false information in open-ended prompts?", "70219e5f-3375-4d8b-97e1-c27c156c5d92": "What are the potential risks associated with confabulated content when using GAI in decision-making applications?", "2a56092e-ece3-4655-95ee-834a051e39d0": "How might GAI outputs mislead users into trusting incorrect information?", "ae149f5d-6e4d-4d1b-bd39-9fe97a61659b": "What are the emerging areas of study related to the deception of humans by LLMs?", "59fdfe3c-2e38-495b-9bf1-c35ac23181c3": "What characteristics define trustworthy AI in the context of managing harmful bias and ensuring safety?", "1ccced82-60ec-4789-b382-915bef9c471a": "What are some examples of content types where creative generation of non-factual content can be a desired behavior?", "8bf29fe3-ace6-4e61-a117-f1b91786a941": "What issue is commonly associated with text-based outputs in relation to confabulations of falsehoods?", "388978b8-f124-4f3e-bbfd-88eebd246825": "What are some potential risks associated with text-to-image models in terms of promoting dangerous messages?", "9293f76e-e677-4b5b-a5ab-6f1adb6b2297": "How do current systems attempt to restrict harmful content generated by GAI, and what limitations do they face?", "82afb579-2549-4c5d-ad63-d0e7f6d9414f": "What are the potential risks associated with \"jailbreaking\" GAI systems in terms of user interactions?", "3ae17a89-471f-4e04-8a26-956a9c769971": "How might negative responses from chatbots impact users who are experiencing distress?", "53effc76-5b76-4104-af7f-ff69ebdff1b8": "What are some risks to privacy associated with GAI systems as mentioned in the context?", "543c0844-ea9b-497a-9173-f75c11d5a03a": "How does the lack of disclosure of data sources by model developers impact user awareness regarding personally identifiable information (PII)?", "cb2aee19-1e33-4178-954a-ab458caa05a0": "What is the problem referred to as data memorization in the context of GAI models?", "777ed501-ec6f-4638-983f-1f8b8ccb5a29": "How can GAI models potentially infer sensitive information that was not included in their training data?", "634f7448-65f1-4d6f-b5ce-e1fa41ec2669": "What are some potential negative impacts of inaccurate inferences made from disparate sources of information?", "405693e9-85c2-49a8-bac3-440b7c32e92a": "How can predictive inferences based on PII or protected attributes lead to adverse decisions for individuals or groups?", "d6fa5c96-5afb-4505-a625-03ff1b0ed2e0": "What are the key characteristics of trustworthy AI mentioned in the context?", "ad3aed5e-31d1-485b-afb7-c68b50336845": "How does the energy consumption of training a single transformer LLM compare to carbon emissions from flights?", "5c979b9a-fcb0-47fa-a18c-b9760f60304e": "What are the differences in energy and carbon intensity between generative and discriminative tasks in LLM inference?", "32a12ffa-280b-43a1-b3a3-256525cfaf49": "What methods can be used to reduce the environmental impacts of trained models during inference?", "d7271f84-7954-4b39-875b-2bff4b19cc5b": "How do current text-to-image models contribute to the underrepresentation of women and racial minorities in professional roles?", "86f74d14-bcd9-4b45-8d56-a6f9142a0e59": "What challenges do image generator models face in producing non-stereotyped content despite specific prompts?", "01055e55-8041-4302-ab80-5f7326633c09": "What are some potential sources of harmful bias in Generative AI (GAI) systems?", "1c97b6d8-b286-4ad1-ba37-8a8bf8e2b700": "How can disparities in model performance across different subgroups contribute to discriminatory decision-making?", "9500b10f-6944-49ec-b7b9-a5aeefff36ad": "How do GAI systems impact the preservation of endangered languages in everyday processes?", "98b5facf-da1f-4aff-85d9-f93fb2a751af": "What challenges do lower-resource languages face in terms of model adoption and accessibility when GAI systems are used?", "9e885eff-4827-4ffb-a42f-09f0c51c3944": "What are the potential consequences of overly homogenized outputs in foundation models?", "8d2c55dd-012a-40f7-9987-0aa940486bf9": "How can model collapse affect the robustness of a new model's outputs?", "9115242f-3967-4986-a565-3d50eeb5c849": "What are the characteristics of trustworthy AI mentioned in the context?", "e597f086-a340-4d51-b32f-a5d6f2b9da75": "How can human perspectives and experiences impact interactions with GAI systems?", "25c9ed22-aa90-44fe-844d-bbc73f145bd2": "What is automation bias and how does it relate to the perception of GAI content quality?", "f0d94129-4a59-4d44-90ed-b011eb11c896": "What are some of the trustworthy AI characteristics mentioned in the context?"}, "relevant_contexts": {"83094a08-78e8-4ba7-acd6-7e180f1e77b9": ["667a3f3a-d8c5-4056-87d4-de7e4f4dc532"], "7b97d512-0aa6-4b2c-aab4-c9a8a6f51a01": ["667a3f3a-d8c5-4056-87d4-de7e4f4dc532"], "c31695f4-3344-41cc-b137-1a9f0d95512a": ["32166c01-9240-42ff-9808-05a270008638"], "39919ba6-319e-419b-9fb0-4a7281db782f": ["32166c01-9240-42ff-9808-05a270008638"], "10c4f2c6-ddc9-4f96-a506-9190fe8f73e8": ["1db6dd97-ab7b-4293-a9ef-1901c95618c2"], "93fe07e5-d45e-4176-9f72-ed2145e6b532": ["1db6dd97-ab7b-4293-a9ef-1901c95618c2"], "26fd8863-3c34-4c79-9ae6-79c9fb954f34": ["4d9998c0-919d-45cc-98bc-f5237b2c83db"], "79c3b833-26fc-4d85-9b6a-c441cbf5800b": ["4d9998c0-919d-45cc-98bc-f5237b2c83db"], "f1425260-20bc-4de7-b6c3-1c21778d5ddb": ["17d1b7aa-407f-453e-8354-40bfe38a7d4c"], "f9734ff1-9752-4725-9b62-a6bf171a2092": ["17d1b7aa-407f-453e-8354-40bfe38a7d4c"], "ad181ee2-5580-4b2e-ae85-55e50eb6e134": ["ec4d6b84-44fc-47a3-9874-fe128d2040e6"], "945ba2fc-e79c-4f1d-9b91-95f4231f3877": ["ec4d6b84-44fc-47a3-9874-fe128d2040e6"], "280a4aba-cc51-4d98-80d7-a66644a61369": ["ee70bb91-b5ce-4306-acde-6e7eca6776a6"], "f14a6c3e-1970-45e2-b873-73d45cc23e53": ["ee70bb91-b5ce-4306-acde-6e7eca6776a6"], "ea6ad41c-e88b-4220-bcb4-7188e8778f9a": ["05668e69-a4ef-4a5e-97f1-9cac096f5524"], "18e6d086-2e28-441f-bd2d-0d40756a1940": ["05668e69-a4ef-4a5e-97f1-9cac096f5524"], "89a1cc96-6404-4f21-803a-ae396c251bc5": ["1f967bdd-8885-434a-bd73-4d9fac6f0d00"], "985eef43-2d4a-41e4-b645-a77cc7ef933e": ["1f967bdd-8885-434a-bd73-4d9fac6f0d00"], "5b2cfb49-5af8-4a83-a99f-9ec77948f2b4": ["b8f1974f-fd76-4906-93ec-d96f6e44252e"], "2ebea0dc-d484-4320-bb48-3e458e2bee2a": ["b8f1974f-fd76-4906-93ec-d96f6e44252e"], "7295441c-8a71-4414-bc3d-39d923cc2b8e": ["647bb4a8-3746-4aee-aac8-9c36dbe269fa"], "4b66d747-273c-45d5-a23c-e4c3997d30b8": ["647bb4a8-3746-4aee-aac8-9c36dbe269fa"], "7ef08335-de05-4c8a-b05a-a43468d8a7d4": ["1c6d1ff9-9fba-420a-8edd-0495bcd860df"], "2da6b6b2-44f0-4e30-8acb-746ad6ff3243": ["1c6d1ff9-9fba-420a-8edd-0495bcd860df"], "d258ad89-cc1d-408d-bd61-e383bd68d9d6": ["9f1acd11-4cd6-4524-ae29-fbd21ee314b6"], "7f330995-62d9-4b2f-bcbb-5d7c524faf02": ["9f1acd11-4cd6-4524-ae29-fbd21ee314b6"], "6b51c17c-b2db-4c5f-9316-46e407628952": ["ddc55e19-e408-4048-8fdc-6871018ff536"], "70768f6b-ce11-484f-8f03-564ca97508c3": ["ddc55e19-e408-4048-8fdc-6871018ff536"], "e883cffb-b753-4281-8408-e5c36dc5d05c": ["bdda3bf9-3f4c-4eec-abfa-33e42deddfac"], "6005b6e6-1cf5-45cd-9dc1-ee20ad69eb67": ["bdda3bf9-3f4c-4eec-abfa-33e42deddfac"], "e9e5a946-c25c-4ef7-ad85-7c4dfeed0baa": ["70189da4-f799-46ec-92e0-53a282d130f5"], "a11db022-ada0-445c-8a76-0a4de548558d": ["70189da4-f799-46ec-92e0-53a282d130f5"], "139ce322-30c2-4fb7-b216-fea6af1ddb90": ["cac17ac9-36fb-45e0-92fb-2ff0e4c294c9"], "3c44db7f-2a21-4d50-9bc2-a51bce531958": ["cac17ac9-36fb-45e0-92fb-2ff0e4c294c9"], "84eed875-d5c0-4d53-beb3-6dce971a018e": ["50493add-dbe4-408a-9761-2b7ba3aaffff"], "29849ab7-4c37-445d-a838-0d31178b9af5": ["50493add-dbe4-408a-9761-2b7ba3aaffff"], "e8385bd9-7530-4acf-8f9a-9c6caab66018": ["f73d41be-7b2f-46d6-a6eb-2c6681dd8ca4"], "c817d4a9-767b-420c-a189-658ba5039699": ["f73d41be-7b2f-46d6-a6eb-2c6681dd8ca4"], "bde37c09-63ee-414f-9ef9-0ff88b6bd024": ["0cbac132-6852-4320-a6d9-d5f195d64648"], "053fd158-e84d-4b66-abfc-03daab5397bd": ["0cbac132-6852-4320-a6d9-d5f195d64648"], "27a3e889-f524-49b1-a2a0-05e645358e39": ["17a0b62b-9679-46ad-a268-db0e09cb103d"], "f8e2935f-305d-4bfb-bdee-f74e384a6329": ["17a0b62b-9679-46ad-a268-db0e09cb103d"], "538baa87-b1a3-4ecc-85d0-683a5766d49f": ["10c6be60-c2e3-47a4-aa74-e737b470f664"], "9733a6fc-abed-4d4c-80af-488a4a2fc61e": ["10c6be60-c2e3-47a4-aa74-e737b470f664"], "b228ced9-37d8-4464-8c22-9d09ed3cfee2": ["5ea67e09-ca49-4115-8731-352d928bf749"], "4af91004-5810-4afc-9979-7b1f732049cb": ["5ea67e09-ca49-4115-8731-352d928bf749"], "08c3137e-14a4-49a2-9eeb-917a4debbb9c": ["11655dea-997f-4040-bd2e-af69d9d0aa2f"], "5e658e0f-d0c4-4e87-b97a-793402c1c23f": ["11655dea-997f-4040-bd2e-af69d9d0aa2f"], "9cbcdba2-b7c6-4cba-8926-32e999f4a3dc": ["3dce6f93-01c8-487f-8277-a5b0d6f3cac2"], "bf5e61ed-e3fe-4a5e-ae1e-5228c57102e1": ["3dce6f93-01c8-487f-8277-a5b0d6f3cac2"], "253a921c-6f61-4a84-bb39-a8a259376547": ["2c9de502-181e-4a23-b329-e7aee0463ac1"], "8dbc8cee-dfab-4302-ad64-eabfb142f2da": ["2c9de502-181e-4a23-b329-e7aee0463ac1"], "76978900-a269-49f0-b2ca-63e64c78c4c8": ["de06e801-89ca-4ac3-b8ea-65e7d576a18a"], "5c529f04-dc09-4aae-ba5c-554650b5d0ea": ["de06e801-89ca-4ac3-b8ea-65e7d576a18a"], "fb529026-96d6-4f2b-9fea-8dc8dbce4740": ["e151843c-efce-4075-b8d3-0449d8e12f01"], "371c096e-e2a9-4b14-9103-cdcfcf685433": ["e151843c-efce-4075-b8d3-0449d8e12f01"], "fc3f78a5-02a3-402f-9299-c279a17eaac3": ["2329f1eb-b7a9-48e5-b443-6ab767ac14a3"], "6319e3d3-5a7f-439c-9c2d-f7dab57dc44b": ["2329f1eb-b7a9-48e5-b443-6ab767ac14a3"], "d1cda133-bdca-4ba3-afab-f3d396f6450b": ["b376a970-2f43-497f-aaea-aa1711f75dcc"], "1fec0800-e2ee-4de8-b5e7-7b15ce37bc67": ["b376a970-2f43-497f-aaea-aa1711f75dcc"], "9bf1b526-dd7e-46fc-b982-69e7f6e3bf41": ["7798b010-bd21-4c0f-b1e4-4e7241fe4336"], "42dffb3e-7170-4bd7-b8ac-f5f02824878c": ["7798b010-bd21-4c0f-b1e4-4e7241fe4336"], "5b988b3e-4836-4c67-bc4d-e7900fedcb2e": ["49f881b2-ceab-42fd-abaf-93ae140afe7b"], "1b210d31-e962-4c3d-befc-27c4874921a3": ["49f881b2-ceab-42fd-abaf-93ae140afe7b"], "5e2b3170-1271-4bbe-a769-914446607627": ["90a9c2e7-b8aa-472e-9ba0-1ce9da377500"], "9a454af9-b0a7-491f-8990-75a6102fc79f": ["90a9c2e7-b8aa-472e-9ba0-1ce9da377500"], "9a5844ef-c9ee-4f74-b8fc-9b11a83a9755": ["f51691b0-4259-4d1f-b739-aed7e8312090"], "fd2a9cbc-3d1b-431a-8e4a-7f8f3950093b": ["f51691b0-4259-4d1f-b739-aed7e8312090"], "70219e5f-3375-4d8b-97e1-c27c156c5d92": ["842712a2-4365-4c50-9e10-0f62d4a61c2d"], "2a56092e-ece3-4655-95ee-834a051e39d0": ["842712a2-4365-4c50-9e10-0f62d4a61c2d"], "ae149f5d-6e4d-4d1b-bd39-9fe97a61659b": ["9a466379-274b-4e12-9738-662b6b7fd709"], "59fdfe3c-2e38-495b-9bf1-c35ac23181c3": ["9a466379-274b-4e12-9738-662b6b7fd709"], "1ccced82-60ec-4789-b382-915bef9c471a": ["03edd116-6f1b-4a2f-b17b-3e1e21ad12bd"], "8bf29fe3-ace6-4e61-a117-f1b91786a941": ["03edd116-6f1b-4a2f-b17b-3e1e21ad12bd"], "388978b8-f124-4f3e-bbfd-88eebd246825": ["824b39a8-fcfb-49a2-8446-636137ff9a14"], "9293f76e-e677-4b5b-a5ab-6f1adb6b2297": ["824b39a8-fcfb-49a2-8446-636137ff9a14"], "82afb579-2549-4c5d-ad63-d0e7f6d9414f": ["a17e7eb2-4fae-4be2-a3cb-8e38e3d8b2e7"], "3ae17a89-471f-4e04-8a26-956a9c769971": ["a17e7eb2-4fae-4be2-a3cb-8e38e3d8b2e7"], "53effc76-5b76-4104-af7f-ff69ebdff1b8": ["a266b8ec-385e-40f5-b2aa-ee2f307e16f4"], "543c0844-ea9b-497a-9173-f75c11d5a03a": ["a266b8ec-385e-40f5-b2aa-ee2f307e16f4"], "cb2aee19-1e33-4178-954a-ab458caa05a0": ["380b60de-f70c-4d16-8af0-a3cdfe15bc8b"], "777ed501-ec6f-4638-983f-1f8b8ccb5a29": ["380b60de-f70c-4d16-8af0-a3cdfe15bc8b"], "634f7448-65f1-4d6f-b5ce-e1fa41ec2669": ["735389c0-a19e-4a44-b8f3-0d85f03697b2"], "405693e9-85c2-49a8-bac3-440b7c32e92a": ["735389c0-a19e-4a44-b8f3-0d85f03697b2"], "d6fa5c96-5afb-4505-a625-03ff1b0ed2e0": ["2b7d3cea-30e1-48ca-b1bd-ca4815a1b59b"], "ad3aed5e-31d1-485b-afb7-c68b50336845": ["2b7d3cea-30e1-48ca-b1bd-ca4815a1b59b"], "5c979b9a-fcb0-47fa-a18c-b9760f60304e": ["dbd1a166-73cc-4d16-8913-8418a1a9a31f"], "32a12ffa-280b-43a1-b3a3-256525cfaf49": ["dbd1a166-73cc-4d16-8913-8418a1a9a31f"], "d7271f84-7954-4b39-875b-2bff4b19cc5b": ["c06b8e65-699c-42c9-81c5-097ccd00b714"], "86f74d14-bcd9-4b45-8d56-a6f9142a0e59": ["c06b8e65-699c-42c9-81c5-097ccd00b714"], "01055e55-8041-4302-ab80-5f7326633c09": ["8f0d347f-6254-4e05-8d07-177a67443c6c"], "1c97b6d8-b286-4ad1-ba37-8a8bf8e2b700": ["8f0d347f-6254-4e05-8d07-177a67443c6c"], "9500b10f-6944-49ec-b7b9-a5aeefff36ad": ["0dae8ef7-5912-46aa-85d0-892907f38f34"], "98b5facf-da1f-4aff-85d9-f93fb2a751af": ["0dae8ef7-5912-46aa-85d0-892907f38f34"], "9e885eff-4827-4ffb-a42f-09f0c51c3944": ["dbee1997-608d-470d-9bb8-ba3056c92098"], "8d2c55dd-012a-40f7-9987-0aa940486bf9": ["dbee1997-608d-470d-9bb8-ba3056c92098"], "9115242f-3967-4986-a565-3d50eeb5c849": ["f16727d7-32cf-4fb7-a7f9-0e289494f2cd"], "e597f086-a340-4d51-b32f-a5d6f2b9da75": ["f16727d7-32cf-4fb7-a7f9-0e289494f2cd"], "25c9ed22-aa90-44fe-844d-bbc73f145bd2": ["677c6fd2-b3c8-4336-b575-fc1f951a8fe5"], "f0d94129-4a59-4d44-90ed-b011eb11c896": ["677c6fd2-b3c8-4336-b575-fc1f951a8fe5"]}, "corpus": {"667a3f3a-d8c5-4056-87d4-de7e4f4dc532": "NIST Trustworthy and Responsible AI  \nNIST AI 600-1 \nArtificial Intelligence Risk Management \nFramework: Generative Artificial \nIntelligence Profile \n \n \n \nThis publication is available free of charge from: \nhttps://doi.org/10.6028/NIST.AI.600-1", "32166c01-9240-42ff-9808-05a270008638": "NIST Trustworthy and Responsible AI  \nNIST AI 600-1 \nArtificial Intelligence Risk Management \nFramework: Generative Artificial \nIntelligence Profile \n \n \n \nThis publication is available free of charge from: \nhttps://doi.org/10.6028/NIST.AI.600-1 \n \nJuly 2024 \n \n \n \n \nU.S. Department of Commerce  \nGina M. Raimondo, Secretary \nNational Institute of Standards and Technology  \nLaurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology", "1db6dd97-ab7b-4293-a9ef-1901c95618c2": "About AI at NIST: The National Institute of Standards and Technology (NIST) develops measurements, \ntechnology, tools, and standards to advance reliable, safe, transparent, explainable, privacy-enhanced, \nand fair arti\ufb01cial intelligence (AI) so that its full commercial and societal bene\ufb01ts can be realized without \nharm to people or the planet. NIST, which has conducted both fundamental and applied work on AI for \nmore than a decade, is also helping to ful\ufb01ll the 2023 Executive Order on Safe, Secure, and Trustworthy \nAI. NIST established the U.S. AI Safety Institute and the companion AI Safety Institute Consortium to \ncontinue the e\ufb00orts set in motion by the E.O. to build the science necessary for safe, secure, and", "4d9998c0-919d-45cc-98bc-f5237b2c83db": "trustworthy development and use of AI. \nAcknowledgments: This report was accomplished with the many helpful comments and contributions \nfrom the community, including the NIST Generative AI Public Working Group, and NIST sta\ufb00 and guest \nresearchers: Chloe Autio, Jesse Dunietz, Patrick Hall, Shomik Jain, Kamie Roberts, Reva Schwartz, Martin \nStanley, and Elham Tabassi. \nNIST Technical Series Policies \nCopyright, Use, and Licensing Statements \nNIST Technical Series Publication Identifier Syntax \nPublication History \nApproved by the NIST Editorial Review Board on 07-25-2024 \nContact Information \nai-inquiries@nist.gov \nNational Institute of Standards and Technology \nAttn: NIST AI Innovation Lab, Information Technology Laboratory", "17d1b7aa-407f-453e-8354-40bfe38a7d4c": "100 Bureau Drive (Mail Stop 8900) Gaithersburg, MD 20899-8900 \nAdditional Information \nAdditional information about this publication and other NIST AI publications are available at \nhttps://airc.nist.gov/Home. \n \nDisclaimer: Certain commercial entities, equipment, or materials may be identi\ufb01ed in this document in \norder to adequately describe an experimental procedure or concept. Such identi\ufb01cation is not intended to \nimply recommendation or endorsement by the National Institute of Standards and Technology, nor is it \nintended to imply that the entities, materials, or equipment are necessarily the best available for the \npurpose. Any mention of commercial, non-pro\ufb01t, academic partners, or their products, or references is", "ec4d6b84-44fc-47a3-9874-fe128d2040e6": "for information only; it is not intended to imply endorsement or recommendation by any U.S. \nGovernment agency.", "ee70bb91-b5ce-4306-acde-6e7eca6776a6": "Table of Contents \n1. \nIntroduction ..............................................................................................................................................1 \n2. \nOverview of Risks Unique to or Exacerbated by GAI .....................................................................2 \n3. \nSuggested Actions to Manage GAI Risks ......................................................................................... 12 \nAppendix A. Primary GAI Considerations ............................................................................................... 47 \nAppendix B. References ................................................................................................................................ 54", "05668e69-a4ef-4a5e-97f1-9cac096f5524": "1 \n1. \nIntroduction \nThis document is a cross-sectoral pro\ufb01le of and companion resource for the AI Risk Management \nFramework (AI RMF 1.0) for Generative AI,1 pursuant to President Biden\u2019s Executive Order (EO) 14110 on \nSafe, Secure, and Trustworthy Arti\ufb01cial Intelligence.2 The AI RMF was released in January 2023, and is \nintended for voluntary use and to improve the ability of organizations to incorporate trustworthiness \nconsiderations into the design, development, use, and evaluation of AI products, services, and systems.  \nA pro\ufb01le is an implementation of the AI RMF functions, categories, and subcategories for a speci\ufb01c \nsetting, application, or technology \u2013 in this case, Generative AI (GAI) \u2013 based on the requirements, risk", "1f967bdd-8885-434a-bd73-4d9fac6f0d00": "tolerance, and resources of the Framework user. AI RMF pro\ufb01les assist organizations in deciding how to \nbest manage AI risks in a manner that is well-aligned with their goals, considers legal/regulatory \nrequirements and best practices, and re\ufb02ects risk management priorities. Consistent with other AI RMF \npro\ufb01les, this pro\ufb01le o\ufb00ers insights into how risk can be managed across various stages of the AI lifecycle \nand for GAI as a technology.  \nAs GAI covers risks of models or applications that can be used across use cases or sectors, this document \nis an AI RMF cross-sectoral pro\ufb01le. Cross-sectoral pro\ufb01les can be used to govern, map, measure, and", "b8f1974f-fd76-4906-93ec-d96f6e44252e": "manage risks associated with activities or business processes common across sectors, such as the use of \nlarge language models (LLMs), cloud-based services, or acquisition. \nThis document de\ufb01nes risks that are novel to or exacerbated by the use of GAI. After introducing and \ndescribing these risks, the document provides a set of suggested actions to help organizations govern, \nmap, measure, and manage these risks. \n \n \n1 EO 14110 de\ufb01nes Generative AI as \u201cthe class of AI models that emulate the structure and characteristics of input \ndata in order to generate derived synthetic content. This can include images, videos, audio, text, and other digital", "647bb4a8-3746-4aee-aac8-9c36dbe269fa": "content.\u201d While not all GAI is derived from foundation models, for purposes of this document, GAI generally refers \nto generative foundation models. The foundation model subcategory of \u201cdual-use foundation models\u201d is de\ufb01ned by \nEO 14110 as \u201can AI model that is trained on broad data; generally uses self-supervision; contains at least tens of \nbillions of parameters; is applicable across a wide range of contexts.\u201d  \n2 This pro\ufb01le was developed per Section 4.1(a)(i)(A) of EO 14110, which directs the Secretary of Commerce, acting \nthrough the Director of the National Institute of Standards and Technology (NIST), to develop a companion \nresource to the AI RMF, NIST AI 100\u20131, for generative AI.", "1c6d1ff9-9fba-420a-8edd-0495bcd860df": "2 \nThis work was informed by public feedback and consultations with diverse stakeholder groups as part of NIST\u2019s \nGenerative AI Public Working Group (GAI PWG). The GAI PWG was an open, transparent, and collaborative \nprocess, facilitated via a virtual workspace, to obtain multistakeholder input on GAI risk management and to \ninform NIST\u2019s approach. \nThe focus of the GAI PWG was limited to four primary considerations relevant to GAI: Governance, Content \nProvenance, Pre-deployment Testing, and Incident Disclosure (further described in Appendix A). As such, the \nsuggested actions in this document primarily address these considerations.", "9f1acd11-4cd6-4524-ae29-fbd21ee314b6": "Future revisions of this pro\ufb01le will include additional AI RMF subcategories, risks, and suggested actions based \non additional considerations of GAI as the space evolves and empirical evidence indicates additional risks. A \nglossary of terms pertinent to GAI risk management will be developed and hosted on NIST\u2019s Trustworthy & \nResponsible AI Resource Center (AIRC), and added to The Language of Trustworthy AI: An In-Depth Glossary of \nTerms. \nThis document was also informed by public comments and consultations from several Requests for Information. \n \n2. \nOverview of Risks Unique to or Exacerbated by GAI \nIn the context of the AI RMF, risk refers to the composite measure of an event\u2019s probability (or", "ddc55e19-e408-4048-8fdc-6871018ff536": "likelihood) of occurring and the magnitude or degree of the consequences of the corresponding event. \nSome risks can be assessed as likely to materialize in a given context, particularly those that have been \nempirically demonstrated in similar contexts. Other risks may be unlikely to materialize in a given \ncontext, or may be more speculative and therefore uncertain. \nAI risks can di\ufb00er from or intensify traditional software risks. Likewise, GAI can exacerbate existing AI \nrisks, and creates unique risks. GAI risks can vary along many dimensions: \n\u2022 \nStage of the AI lifecycle: Risks can arise during design, development, deployment, operation, \nand/or decommissioning. \n\u2022", "bdda3bf9-3f4c-4eec-abfa-33e42deddfac": "\u2022 \nScope: Risks may exist at individual model or system levels, at the application or implementation \nlevels (i.e., for a speci\ufb01c use case), or at the ecosystem level \u2013 that is, beyond a single system or \norganizational context. Examples of the latter include the expansion of \u201calgorithmic \nmonocultures,3\u201d resulting from repeated use of the same model, or impacts on access to \nopportunity, labor markets, and the creative economies.4 \n\u2022 \nSource of risk: Risks may emerge from factors related to the design, training, or operation of the \nGAI model itself, stemming in some cases from GAI model or system inputs, and in other cases, \nfrom GAI system outputs. Many GAI risks, however, originate from human behavior, including", "70189da4-f799-46ec-92e0-53a282d130f5": "3 \u201cAlgorithmic monocultures\u201d refers to the phenomenon in which repeated use of the same model or algorithm in \nconsequential decision-making settings like employment and lending can result in increased susceptibility by \nsystems to correlated failures (like unexpected shocks), due to multiple actors relying on the same algorithm.  \n4 Many studies have projected the impact of AI on the workforce and labor markets. Fewer studies have examined \nthe impact of GAI on the labor market, though some industry surveys indicate that that both employees and \nemployers are pondering this disruption.", "cac17ac9-36fb-45e0-92fb-2ff0e4c294c9": "3 \nthe abuse, misuse, and unsafe repurposing by humans (adversarial or not), and others result \nfrom interactions between a human and an AI system.  \n\u2022 \nTime scale: GAI risks may materialize abruptly or across extended periods. Examples include \nimmediate (and/or prolonged) emotional harm and potential risks to physical safety due to the \ndistribution of harmful deepfake images, or the long-term e\ufb00ect of disinformation on societal \ntrust in public institutions. \nThe presence of risks and where they fall along the dimensions above will vary depending on the \ncharacteristics of the GAI model, system, or use case at hand. These characteristics include but are not", "50493add-dbe4-408a-9761-2b7ba3aaffff": "limited to GAI model or system architecture, training mechanisms and libraries, data types used for \ntraining or \ufb01ne-tuning, levels of model access or availability of model weights, and application or use \ncase context. \nOrganizations may choose to tailor how they measure GAI risks based on these characteristics. They may \nadditionally wish to allocate risk management resources relative to the severity and likelihood of \nnegative impacts, including where and how these risks manifest, and their direct and material impacts \nharms in the context of GAI use. Mitigations for model or system level risks may di\ufb00er from mitigations \nfor use-case or ecosystem level risks.", "f73d41be-7b2f-46d6-a6eb-2c6681dd8ca4": "Importantly, some GAI risks are unknown, and are therefore di\ufb03cult to properly scope or evaluate given \nthe uncertainty about potential GAI scale, complexity, and capabilities. Other risks may be known but \ndi\ufb03cult to estimate given the wide range of GAI stakeholders, uses, inputs, and outputs. Challenges with \nrisk estimation are aggravated by a lack of visibility into GAI training data, and the generally immature \nstate of the science of AI measurement and safety today. This document focuses on risks for which there \nis an existing empirical evidence base at the time this pro\ufb01le was written; for example, speculative risks \nthat may potentially arise in more advanced, future GAI systems are not considered. Future updates may", "0cbac132-6852-4320-a6d9-d5f195d64648": "incorporate additional risks or provide further details on the risks identi\ufb01ed below. \nTo guide organizations in identifying and managing GAI risks, a set of risks unique to or exacerbated by \nthe development and use of GAI are de\ufb01ned below.5 Each risk is labeled according to the outcome, \nobject, or source of the risk (i.e., some are risks \u201cto\u201d a subject or domain and others are risks \u201cof\u201d or \n\u201cfrom\u201d an issue or theme). These risks provide a lens through which organizations can frame and execute \nrisk management e\ufb00orts. To help streamline risk management e\ufb00orts, each risk is mapped in Section 3 \n(as well as in tables in Appendix B) to relevant Trustworthy AI Characteristics identi\ufb01ed in the AI RMF.", "17a0b62b-9679-46ad-a268-db0e09cb103d": "5 These risks can be further categorized by organizations depending on their unique approaches to risk de\ufb01nition \nand management. One possible way to further categorize these risks, derived in part from the UK\u2019s International \nScienti\ufb01c Report on the Safety of Advanced AI, could be: 1) Technical / Model risks (or risk from malfunction): \nConfabulation; Dangerous or Violent Recommendations; Data Privacy; Value Chain and Component Integration; \nHarmful Bias, and Homogenization; 2) Misuse by humans (or malicious use): CBRN Information or Capabilities; \nData Privacy; Human-AI Con\ufb01guration; Obscene, Degrading, and/or Abusive Content; Information Integrity;", "10c6be60-c2e3-47a4-aa74-e737b470f664": "Information Security; 3) Ecosystem / societal risks (or systemic risks): Data Privacy; Environmental; Intellectual \nProperty. We also note that some risks are cross-cutting between these categories.", "5ea67e09-ca49-4115-8731-352d928bf749": "4 \n1. CBRN Information or Capabilities: Eased access to or synthesis of materially nefarious \ninformation or design capabilities related to chemical, biological, radiological, or nuclear (CBRN) \nweapons or other dangerous materials or agents. \n2. Confabulation: The production of con\ufb01dently stated but erroneous or false content (known \ncolloquially as \u201challucinations\u201d or \u201cfabrications\u201d) by which users may be misled or deceived.6 \n3. Dangerous, Violent, or Hateful Content: Eased production of and access to violent, inciting, \nradicalizing, or threatening content as well as recommendations to carry out self-harm or \nconduct illegal activities. Includes di\ufb03culty controlling public exposure to hateful and disparaging \nor stereotyping content.", "11655dea-997f-4040-bd2e-af69d9d0aa2f": "4. Data Privacy: Impacts due to leakage and unauthorized use, disclosure, or de-anonymization of \nbiometric, health, location, or other personally identi\ufb01able information or sensitive data.7 \n5. Environmental Impacts: Impacts due to high compute resource utilization in training or \noperating GAI models, and related outcomes that may adversely impact ecosystems.  \n6. Harmful Bias or Homogenization: Ampli\ufb01cation and exacerbation of historical, societal, and \nsystemic biases; performance disparities8 between sub-groups or languages, possibly due to \nnon-representative training data, that result in discrimination, ampli\ufb01cation of biases, or \nincorrect presumptions about performance; undesired homogeneity that skews system or model", "3dce6f93-01c8-487f-8277-a5b0d6f3cac2": "outputs, which may be erroneous, lead to ill-founded decision-making, or amplify harmful \nbiases.  \n7. Human-AI Con\ufb01guration: Arrangements of or interactions between a human and an AI system \nwhich can result in the human inappropriately anthropomorphizing GAI systems or experiencing \nalgorithmic aversion, automation bias, over-reliance, or emotional entanglement with GAI \nsystems. \n8. Information Integrity: Lowered barrier to entry to generate and support the exchange and \nconsumption of content which may not distinguish fact from opinion or \ufb01ction or acknowledge \nuncertainties, or could be leveraged for large-scale dis- and mis-information campaigns.", "2c9de502-181e-4a23-b329-e7aee0463ac1": "9. Information Security: Lowered barriers for o\ufb00ensive cyber capabilities, including via automated \ndiscovery and exploitation of vulnerabilities to ease hacking, malware, phishing, o\ufb00ensive cyber \n \n \n6 Some commenters have noted that the terms \u201challucination\u201d and \u201cfabrication\u201d anthropomorphize GAI, which \nitself is a risk related to GAI systems as it can inappropriately attribute human characteristics to non-human \nentities.  \n7 What is categorized as sensitive data or sensitive PII can be highly contextual based on the nature of the \ninformation, but examples of sensitive information include information that relates to an information subject\u2019s \nmost intimate sphere, including political opinions, sex life, or criminal convictions.", "de06e801-89ca-4ac3-b8ea-65e7d576a18a": "8 The notion of harm presumes some baseline scenario that the harmful factor (e.g., a GAI model) makes worse. \nWhen the mechanism for potential harm is a disparity between groups, it can be di\ufb03cult to establish what the \nmost appropriate baseline is to compare against, which can result in divergent views on when a disparity between \nAI behaviors for di\ufb00erent subgroups constitutes a harm. In discussing harms from disparities such as biased \nbehavior, this document highlights examples where someone\u2019s situation is worsened relative to what it would have \nbeen in the absence of any AI system, making the outcome unambiguously a harm of the system.", "e151843c-efce-4075-b8d3-0449d8e12f01": "5 \noperations, or other cyberattacks; increased attack surface for targeted cyberattacks, which may \ncompromise a system\u2019s availability or the con\ufb01dentiality or integrity of training data, code, or \nmodel weights.  \n10. Intellectual Property: Eased production or replication of alleged copyrighted, trademarked, or \nlicensed content without authorization (possibly in situations which do not fall under fair use); \neased exposure of trade secrets; or plagiarism or illegal replication.  \n11. Obscene, Degrading, and/or Abusive Content: Eased production of and access to obscene, \ndegrading, and/or abusive imagery which can cause harm, including synthetic child sexual abuse \nmaterial (CSAM), and nonconsensual intimate images (NCII) of adults.", "2329f1eb-b7a9-48e5-b443-6ab767ac14a3": "12. Value Chain and Component Integration: Non-transparent or untraceable integration of \nupstream third-party components, including data that has been improperly obtained or not \nprocessed and cleaned due to increased automation from GAI; improper supplier vetting across \nthe AI lifecycle; or other issues that diminish transparency or accountability for downstream \nusers. \n2.1. CBRN Information or Capabilities \nIn the future, GAI may enable malicious actors to more easily access CBRN weapons and/or relevant \nknowledge, information, materials, tools, or technologies that could be misused to assist in the design, \ndevelopment, production, or use of CBRN weapons or other dangerous materials or agents. While", "b376a970-2f43-497f-aaea-aa1711f75dcc": "relevant biological and chemical threat knowledge and information is often publicly accessible, LLMs \ncould facilitate its analysis or synthesis, particularly by individuals without formal scienti\ufb01c training or \nexpertise.  \nRecent research on this topic found that LLM outputs regarding biological threat creation and attack \nplanning provided minimal assistance beyond traditional search engine queries, suggesting that state-of-\nthe-art LLMs at the time these studies were conducted do not substantially increase the operational \nlikelihood of such an attack. The physical synthesis development, production, and use of chemical or \nbiological agents will continue to require both applicable expertise and supporting materials and", "7798b010-bd21-4c0f-b1e4-4e7241fe4336": "infrastructure. The impact of GAI on chemical or biological agent misuse will depend on what the key \nbarriers for malicious actors are (e.g., whether information access is one such barrier), and how well GAI \ncan help actors address those barriers.  \nFurthermore, chemical and biological design tools (BDTs) \u2013 highly specialized AI systems trained on \nscienti\ufb01c data that aid in chemical and biological design \u2013 may augment design capabilities in chemistry \nand biology beyond what text-based LLMs are able to provide. As these models become more \ne\ufb03cacious, including for bene\ufb01cial uses, it will be important to assess their potential to be used for \nharm, such as the ideation and design of novel harmful chemical or biological agents.", "49f881b2-ceab-42fd-abaf-93ae140afe7b": "While some of these described capabilities lie beyond the reach of existing GAI tools, ongoing \nassessments of this risk would be enhanced by monitoring both the ability of AI tools to facilitate CBRN \nweapons planning and GAI systems\u2019 connection or access to relevant data and tools. \nTrustworthy AI Characteristic: Safe, Explainable and Interpretable", "90a9c2e7-b8aa-472e-9ba0-1ce9da377500": "6 \n2.2. Confabulation \n\u201cConfabulation\u201d refers to a phenomenon in which GAI systems generate and con\ufb01dently present \nerroneous or false content in response to prompts. Confabulations also include generated outputs that \ndiverge from the prompts or other input or that contradict previously generated statements in the same \ncontext. These phenomena are colloquially also referred to as \u201challucinations\u201d or \u201cfabrications.\u201d \nConfabulations can occur across GAI outputs and contexts.9,10 Confabulations are a natural result of the \nway generative models are designed: they generate outputs that approximate the statistical distribution \nof their training data; for example, LLMs predict the next token or word in a sentence or phrase. While", "f51691b0-4259-4d1f-b739-aed7e8312090": "such statistical prediction can produce factually accurate and consistent outputs, it can also produce \noutputs that are factually inaccurate or internally inconsistent. This dynamic is particularly relevant when \nit comes to open-ended prompts for long-form responses and in domains which require highly \ncontextual and/or domain expertise.  \nRisks from confabulations may arise when users believe false content \u2013 often due to the con\ufb01dent nature \nof the response \u2013 leading users to act upon or promote the false information. This poses a challenge for \nmany real-world applications, such as in healthcare, where a confabulated summary of patient \ninformation reports could cause doctors to make incorrect diagnoses and/or recommend the wrong", "842712a2-4365-4c50-9e10-0f62d4a61c2d": "treatments. Risks of confabulated content may be especially important to monitor when integrating GAI \ninto applications involving consequential decision making. \nGAI outputs may also include confabulated logic or citations that purport to justify or explain the \nsystem\u2019s answer, which may further mislead humans into inappropriately trusting the system\u2019s output. \nFor instance, LLMs sometimes provide logical steps for how they arrived at an answer even when the \nanswer itself is incorrect. Similarly, an LLM could falsely assert that it is human or has human traits, \npotentially deceiving humans into believing they are speaking with another human.", "9a466379-274b-4e12-9738-662b6b7fd709": "The extent to which humans can be deceived by LLMs, the mechanisms by which this may occur, and the \npotential risks from adversarial prompting of such behavior are emerging areas of study. Given the wide \nrange of downstream impacts of GAI, it is di\ufb03cult to estimate the downstream scale and impact of \nconfabulations. \nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Safe, Valid and Reliable, Explainable \nand Interpretable \n2.3. Dangerous, Violent, or Hateful Content \nGAI systems can produce content that is inciting, radicalizing, or threatening, or that glori\ufb01es violence, \nwith greater ease and scale than other technologies. LLMs have been reported to generate dangerous or", "03edd116-6f1b-4a2f-b17b-3e1e21ad12bd": "violent recommendations, and some models have generated actionable instructions for dangerous or \n \n \n9 Confabulations of falsehoods are most commonly a problem for text-based outputs; for audio, image, or video \ncontent, creative generation of non-factual content can be a desired behavior.  \n10 For example, legal confabulations have been shown to be pervasive in current state-of-the-art LLMs. See also, \ne.g.,", "824b39a8-fcfb-49a2-8446-636137ff9a14": "7 \nunethical behavior. Text-to-image models also make it easy to create images that could be used to \npromote dangerous or violent messages. Similar concerns are present for other GAI media, including \nvideo and audio. GAI may also produce content that recommends self-harm or criminal/illegal activities.  \nMany current systems restrict model outputs to limit certain content or in response to certain prompts, \nbut this approach may still produce harmful recommendations in response to other less-explicit, novel \nprompts (also relevant to CBRN Information or Capabilities, Data Privacy, Information Security, and \nObscene, Degrading and/or Abusive Content). Crafting such prompts deliberately is known as", "a17e7eb2-4fae-4be2-a3cb-8e38e3d8b2e7": "\u201cjailbreaking,\u201d or, manipulating prompts to circumvent output controls. Limitations of GAI systems can be \nharmful or dangerous in certain contexts. Studies have observed that users may disclose mental health \nissues in conversations with chatbots \u2013 and that users exhibit negative reactions to unhelpful responses \nfrom these chatbots during situations of distress. \nThis risk encompasses di\ufb03culty controlling creation of and public exposure to o\ufb00ensive or hateful \nlanguage, and denigrating or stereotypical content generated by AI. This kind of speech may contribute \nto downstream harm such as fueling dangerous or violent behaviors. The spread of denigrating or", "a266b8ec-385e-40f5-b2aa-ee2f307e16f4": "stereotypical content can also further exacerbate representational harms (see Harmful Bias and \nHomogenization below).  \nTrustworthy AI Characteristics: Safe, Secure and Resilient \n2.4. Data Privacy \nGAI systems raise several risks to privacy. GAI system training requires large volumes of data, which in \nsome cases may include personal data. The use of personal data for GAI training raises risks to widely \naccepted privacy principles, including to transparency, individual participation (including consent), and \npurpose speci\ufb01cation. For example, most model developers do not disclose speci\ufb01c data sources on \nwhich models were trained, limiting user awareness of whether personally identi\ufb01ably information (PII)", "380b60de-f70c-4d16-8af0-a3cdfe15bc8b": "was trained on and, if so, how it was collected.  \nModels may leak, generate, or correctly infer sensitive information about individuals. For example, \nduring adversarial attacks, LLMs have revealed sensitive information (from the public domain) that was \nincluded in their training data. This problem has been referred to as data memorization, and may pose \nexacerbated privacy risks even for data present only in a small number of training samples.  \nIn addition to revealing sensitive information in GAI training data, GAI models may be able to correctly \ninfer PII or sensitive data that was not in their training data nor disclosed by the user by stitching", "735389c0-a19e-4a44-b8f3-0d85f03697b2": "together information from disparate sources. These inferences can have negative impact on an individual \neven if the inferences are not accurate (e.g., confabulations), and especially if they reveal information \nthat the individual considers sensitive or that is used to disadvantage or harm them. \nBeyond harms from information exposure (such as extortion or dignitary harm), wrong or inappropriate \ninferences of PII can contribute to downstream or secondary harmful impacts. For example, predictive \ninferences made by GAI models based on PII or protected attributes can contribute to adverse decisions, \nleading to representational or allocative harms to individuals or groups (see Harmful Bias and \nHomogenization below).", "2b7d3cea-30e1-48ca-b1bd-ca4815a1b59b": "8 \nTrustworthy AI Characteristics: Accountable and Transparent, Privacy Enhanced, Safe, Secure and \nResilient \n2.5. Environmental Impacts \nTraining, maintaining, and operating (running inference on) GAI systems are resource-intensive activities, \nwith potentially large energy and environmental footprints. Energy and carbon emissions vary based on \nwhat is being done with the GAI model (i.e., pre-training, \ufb01ne-tuning, inference), the modality of the \ncontent, hardware used, and type of task or application. \nCurrent estimates suggest that training a single transformer LLM can emit as much carbon as 300 round-\ntrip \ufb02ights between San Francisco and New York. In a study comparing energy consumption and carbon", "dbd1a166-73cc-4d16-8913-8418a1a9a31f": "emissions for LLM inference, generative tasks (e.g., text summarization) were found to be more energy- \nand carbon-intensive than discriminative or non-generative tasks (e.g., text classi\ufb01cation).  \nMethods for creating smaller versions of trained models, such as model distillation or compression, \ncould reduce environmental impacts at inference time, but training and tuning such models may still \ncontribute to their environmental impacts. Currently there is no agreed upon method to estimate \nenvironmental impacts from GAI.  \nTrustworthy AI Characteristics: Accountable and Transparent, Safe \n2.6. Harmful Bias and Homogenization \nBias exists in many forms and can become ingrained in automated systems. AI systems, including GAI", "c06b8e65-699c-42c9-81c5-097ccd00b714": "systems, can increase the speed and scale at which harmful biases manifest and are acted upon, \npotentially perpetuating and amplifying harms to individuals, groups, communities, organizations, and \nsociety. For example, when prompted to generate images of CEOs, doctors, lawyers, and judges, current \ntext-to-image models underrepresent women and/or racial minorities, and people with disabilities. \nImage generator models have also produced biased or stereotyped output for various demographic \ngroups and have di\ufb03culty producing non-stereotyped content even when the prompt speci\ufb01cally \nrequests image features that are inconsistent with the stereotypes. Harmful bias in GAI models, which", "8f0d347f-6254-4e05-8d07-177a67443c6c": "may stem from their training data, can also cause representational harms or perpetuate or exacerbate \nbias based on race, gender, disability, or other protected classes.  \nHarmful bias in GAI systems can also lead to harms via disparities between how a model performs for \ndi\ufb00erent subgroups or languages (e.g., an LLM may perform less well for non-English languages or \ncertain dialects). Such disparities can contribute to discriminatory decision-making or ampli\ufb01cation of \nexisting societal biases. In addition, GAI systems may be inappropriately trusted to perform similarly \nacross all subgroups, which could leave the groups facing underperformance with worse outcomes than", "0dae8ef7-5912-46aa-85d0-892907f38f34": "if no GAI system were used. Disparate or reduced performance for lower-resource languages also \npresents challenges to model adoption, inclusion, and accessibility, and may make preservation of \nendangered languages more di\ufb03cult if GAI systems become embedded in everyday processes that would \notherwise have been opportunities to use these languages.  \nBias is mutually reinforcing with the problem of undesired homogenization, in which GAI systems \nproduce skewed distributions of outputs that are overly uniform (for example, repetitive aesthetic styles", "dbee1997-608d-470d-9bb8-ba3056c92098": "9 \nand reduced content diversity). Overly homogenized outputs can themselves be incorrect, or they may \nlead to unreliable decision-making or amplify harmful biases. These phenomena can \ufb02ow from \nfoundation models to downstream models and systems, with the foundation models acting as \n\u201cbottlenecks,\u201d or single points of failure.  \nOverly homogenized content can contribute to \u201cmodel collapse.\u201d Model collapse can occur when model \ntraining over-relies on synthetic data, resulting in data points disappearing from the distribution of the \nnew model\u2019s outputs. In addition to threatening the robustness of the model overall, model collapse \ncould lead to homogenized outputs, including by amplifying any homogenization from the model used to", "f16727d7-32cf-4fb7-a7f9-0e289494f2cd": "generate the synthetic training data. \nTrustworthy AI Characteristics: Fair with Harmful Bias Managed, Valid and Reliable \n2.7. Human-AI Con\ufb01guration \nGAI system use can involve varying risks of miscon\ufb01gurations and poor interactions between a system \nand a human who is interacting with it. Humans bring their unique perspectives, experiences, or domain-\nspeci\ufb01c expertise to interactions with AI systems but may not have detailed knowledge of AI systems and \nhow they work. As a result, human experts may be unnecessarily \u201caverse\u201d to GAI systems, and thus \ndeprive themselves or others of GAI\u2019s bene\ufb01cial uses.  \nConversely, due to the complexity and increasing reliability of GAI technology, over time, humans may", "677c6fd2-b3c8-4336-b575-fc1f951a8fe5": "over-rely on GAI systems or may unjusti\ufb01ably perceive GAI content to be of higher quality than that \nproduced by other sources. This phenomenon is an example of automation bias, or excessive deference \nto automated systems. Automation bias can exacerbate other risks of GAI, such as risks of confabulation \nor risks of bias or homogenization. \nThere may also be concerns about emotional entanglement between humans and GAI systems, which \ncould lead to negative psychological impacts. \nTrustworthy AI Characteristics: Accountable and Transparent, Explainable and Interpretable, Fair with \nHarmful Bias Managed, Privacy Enhanced, Safe, Valid and Reliable \n2.8. Information Integrity"}}